{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kqYXnCUOeRS-w9EIekSRj5g9TNGD1pLh","timestamp":1764092538402},{"file_id":"1vqqaFGJLxAFj4hV3_z-WBf8DmozChE9R","timestamp":1764091619112}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Training pipeline: Qwen2-VL-2B Fine-tuning for GLSL Generation\n","\n","**Goal:** Fine-tune Qwen2-VL-2B-Instruct to generate GLSL shader code from rendered images.\n","\n","**Strategy:** Start simple, iterate on stability."],"metadata":{"id":"header"}},{"cell_type":"code","source":["# @title Setup: Mount Drive & Install Dependencies\n","\n","import os\n","import sys\n","\n","# 1. Mount Google Drive\n","from google.colab import drive, userdata\n","print(\"[SYSTEM] Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\" Drive mounted\")\n","\n","# 2. Install core dependencies with compatible versions\n","print(\"\\n[SETUP] Installing dependencies...\")\n","!pip install -q transformers==4.45.0 accelerate==0.34.0 peft==0.12.0 datasets==2.20.0 pillow tqdm\n","\n","print(\"\\n Setup complete\")\n","print(f\" PyTorch: {__import__('torch').__version__}\")\n","print(f\" Transformers: {__import__('transformers').__version__}\")\n","print(f\" GPU Available: {__import__('torch').cuda.is_available()}\")"],"metadata":{"id":"setup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Configuration\n","\n","CONFIG = {\n","    # Paths\n","    \"dataset_dir\": \"/content/drive/My Drive/projects/EarthShader/dataset\",\n","    \"output_dir\": \"/content/drive/My Drive/projects/EarthShader/checkpoints_training\", # Changed folder to avoid overwriting\n","\n","    # Model\n","    \"model_name\": \"Qwen/Qwen2-VL-2B-Instruct\",\n","    # \"model_id\": \"Qwen/Qwen2-VL-7B-Instruct\", # Uncomment for 7B run\n","    \"max_seq_length\": 1024,\n","\n","    # Training\n","    \"num_train_epochs\": 15,            # Increased from 3 to 15\n","    \"per_device_train_batch_size\": 1,\n","    \"gradient_accumulation_steps\": 8,\n","    \"gradient_checkpointing\": True,\n","    \"learning_rate\": 1e-5,             # Lowered from 2e-5 for stability\n","    \"warmup_steps\": 50,\n","    \"logging_steps\": 10,\n","    \"save_steps\": 200,                 # Save less frequently\n","    \"max_samples\": None,\n","\n","    # LoRA\n","    \"use_lora\": True,\n","    \"lora_r\": 8,\n","    \"lora_alpha\": 16,\n","    \"lora_dropout\": 0.05,\n","}\n","\n","# Create output directory\n","os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n","print(f\" Configuration loaded for Training Pipeline\")"],"metadata":{"id":"config"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Load Dataset\n","\n","import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from tqdm import tqdm\n","\n","class ShaderDataset(Dataset):\n","    def __init__(self, dataset_dir, max_samples=None):\n","        self.dataset_dir = dataset_dir\n","\n","        # Find all image files\n","        all_files = os.listdir(dataset_dir)\n","        image_files = sorted([f for f in all_files if f.endswith('.jpg')])\n","\n","        if max_samples:\n","            image_files = image_files[:max_samples]\n","\n","        # Verify pairs exist\n","        self.samples = []\n","        for img_file in tqdm(image_files, desc=\"Validating dataset\"):\n","            base_name = img_file.replace('.jpg', '')\n","            glsl_file = base_name + '.glsl'\n","\n","            img_path = os.path.join(dataset_dir, img_file)\n","            glsl_path = os.path.join(dataset_dir, glsl_file)\n","\n","            if os.path.exists(img_path) and os.path.exists(glsl_path):\n","                self.samples.append({\n","                    'image_path': img_path,\n","                    'glsl_path': glsl_path\n","                })\n","\n","        print(f\" Loaded {len(self.samples)} valid pairs\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        sample = self.samples[idx]\n","\n","        # Load image\n","        image = Image.open(sample['image_path']).convert('RGB')\n","\n","        # Load GLSL code\n","        with open(sample['glsl_path'], 'r') as f:\n","            glsl_code = f.read()\n","\n","        return {\n","            'image': image,\n","            'glsl_code': glsl_code\n","        }\n","\n","# Load dataset\n","print(\"[DATA] Loading dataset...\")\n","dataset = ShaderDataset(\n","    CONFIG[\"dataset_dir\"],\n","    max_samples=CONFIG[\"max_samples\"]\n",")\n","\n","# Show sample\n","if len(dataset) > 0:\n","    sample = dataset[0]\n","    print(f\"\\n Sample check:\")\n","    print(f\"  Image size: {sample['image'].size}\")\n","    print(f\"  Code length: {len(sample['glsl_code'])} chars\")\n","    print(f\"  Code preview: {sample['glsl_code'][:100]}...\")"],"metadata":{"id":"dataset"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Load Model & Processor\n","\n","import torch\n","from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n","from peft import LoraConfig, get_peft_model\n","\n","print(\"[MODEL] Loading Qwen2-VL-7B-Instruct...\")\n","\n","# Load processor\n","processor = AutoProcessor.from_pretrained(\n","    CONFIG[\"model_name\"],\n","    trust_remote_code=True\n",")\n","print(\" Processor loaded\")\n","\n","# Load model in float16 (no quantization)\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    CONFIG[\"model_name\"],\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","# Enable gradient checkpointing to save memory\n","if CONFIG.get(\"gradient_checkpointing\", False):\n","    model.gradient_checkpointing_enable()\n","    print(\" Gradient checkpointing enabled\")\n","\n","print(\" Model loaded\")\n","\n","# Apply LoRA (without quantization prep)\n","if CONFIG[\"use_lora\"]:\n","    print(\"\\n[LORA] Applying LoRA adapters...\")\n","\n","    # Make model trainable\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    lora_config = LoraConfig(\n","        r=CONFIG[\"lora_r\"],\n","        lora_alpha=CONFIG[\"lora_alpha\"],\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","        lora_dropout=CONFIG[\"lora_dropout\"],\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\"\n","    )\n","\n","    model = get_peft_model(model, lora_config)\n","    model.print_trainable_parameters()\n","    print(\" LoRA applied\")\n","\n","print(f\"\\n Model ready on {model.device}\")"],"metadata":{"id":"model"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Prepare Training Data\n","\n","from torch.utils.data import DataLoader\n","import torch\n","\n","def collate_fn(batch):\n","    images = []\n","    full_texts = []\n","    prompt_texts = []\n","\n","    for item in batch:\n","        images.append(item['image'])\n","\n","        # Important: Use a specific prompt.\n","        # We explicitly ask for \"Shadertoy-style\" and \"void mainImage\"\n","        # This prevents the model from generating C++ setup code.\n","        prompt = \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Generate Shadertoy-style GLSL code to render this image. The code should start with void mainImage.<|im_end|>\\n<|im_start|>assistant\\n\"\n","\n","        full_text = prompt + item['glsl_code'] + \"<|im_end|>\"\n","\n","        full_texts.append(full_text)\n","        prompt_texts.append(prompt)\n","\n","    # Process full text (Input)\n","    inputs = processor(\n","        text=full_texts,\n","        images=images,\n","        padding=True,\n","        truncation=True,\n","        max_length=CONFIG[\"max_seq_length\"],\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Process prompt only (for Masking)\n","    inputs_prompts = processor(\n","        text=prompt_texts,\n","        images=images,\n","        padding=True,\n","        truncation=True,\n","        max_length=CONFIG[\"max_seq_length\"],\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Create Labels with Masking\n","    labels = inputs[\"input_ids\"].clone()\n","    for i in range(len(batch)):\n","        prompt_len = inputs_prompts[\"attention_mask\"][i].sum().item()\n","        labels[i, :prompt_len] = -100\n","\n","    inputs[\"labels\"] = labels\n","    return inputs\n","\n","# Create DataLoader\n","train_dataloader = DataLoader(\n","    dataset,\n","    batch_size=CONFIG[\"per_device_train_batch_size\"],\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    num_workers=0\n",")\n","\n","print(f\" Training Pipeline DataLoader ready\")"],"metadata":{"id":"dataloader"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Training Loop\n","\n","import torch\n","from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","\n","print(\"[TRAIN] Starting training...\\n\")\n","\n","# Setup optimizer\n","optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n","\n","# Calculate total steps\n","num_training_steps = len(train_dataloader) * CONFIG[\"num_train_epochs\"] // CONFIG[\"gradient_accumulation_steps\"]\n","\n","# Setup scheduler\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=CONFIG[\"warmup_steps\"],\n","    num_training_steps=num_training_steps\n",")\n","\n","# Training state\n","model.train()\n","global_step = 0\n","total_loss = 0\n","\n","# Training loop\n","for epoch in range(CONFIG[\"num_train_epochs\"]):\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Epoch {epoch + 1}/{CONFIG['num_train_epochs']}\")\n","    print(f\"{'='*60}\\n\")\n","\n","    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n","\n","    for step, batch in enumerate(progress_bar):\n","        # Move batch to device\n","        batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v\n","                for k, v in batch.items()}\n","\n","        # Forward pass\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","\n","        # Scale loss for gradient accumulation\n","        loss = loss / CONFIG[\"gradient_accumulation_steps\"]\n","        loss.backward()\n","\n","        total_loss += loss.item()\n","\n","        # Update weights\n","        if (step + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","            global_step += 1\n","\n","            # Logging\n","            if global_step % CONFIG[\"logging_steps\"] == 0:\n","                avg_loss = total_loss / CONFIG[\"logging_steps\"]\n","                progress_bar.set_postfix({\n","                    'loss': f'{avg_loss:.4f}',\n","                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n","                })\n","                total_loss = 0\n","\n","            # Save checkpoint\n","            if global_step % CONFIG[\"save_steps\"] == 0:\n","                checkpoint_dir = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint-{global_step}\")\n","                print(f\"\\n Saving checkpoint to {checkpoint_dir}\")\n","                model.save_pretrained(checkpoint_dir)\n","                processor.save_pretrained(checkpoint_dir)\n","\n","# Final save\n","final_dir = os.path.join(CONFIG[\"output_dir\"], \"final_model\")\n","print(f\"\\n Saving final model to {final_dir}\")\n","model.save_pretrained(final_dir)\n","processor.save_pretrained(final_dir)\n","\n","print(\"\\n Training complete!\")"],"metadata":{"id":"training"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Test Inference\n","import random\n","import torch\n","from PIL import Image\n","\n","print(\"[TEST] Running Inference...\\n\")\n","\n","# Get random test sample\n","test_sample = random.choice(dataset)\n","test_image = test_sample['image']\n","true_code = test_sample['glsl_code']\n","\n","# Critical: Use the exact same prompt as in training.\n","# TODO(stefan): Make this a constant for reuse.\n","text = \"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Generate Shadertoy-style GLSL code to render this image. The code should start with void mainImage.<|im_end|>\\n<|im_start|>assistant\\n\"\n","\n","inputs = processor(\n","    text=[text],\n","    images=[test_image],\n","    return_tensors=\"pt\"\n",").to(model.device)\n","\n","# Generate with sampling (creativity) to break loops\n","model.eval()\n","with torch.no_grad():\n","    output_ids = model.generate(\n","        **inputs,\n","        max_new_tokens=1024,\n","        do_sample=True,\n","        temperature=0.6,         # Slightly lower temp for code precision\n","        top_p=0.9,\n","        repetition_penalty=1.15  # Prevent \"void main\" loops\n","    )\n","\n","# Decode results\n","generated_ids = output_ids[0][inputs['input_ids'].shape[1]:]\n","generated_text = processor.decode(generated_ids, skip_special_tokens=True)\n","\n","# Display results\n","print(\"=\" * 60)\n","print(\"GENERATED SHADERTOY CODE:\")\n","print(\"=\" * 60)\n","print(generated_text)\n","print(\"\\n\" + \"=\" * 60)\n","print(\"TRUE CODE snippet:\")\n","print(\"=\" * 60)\n","print(true_code[:300] + \"...\")\n","\n","# Show image\n","from IPython.display import display\n","print(\"\\nTest Image:\")\n","display(test_image.resize((256, 256)))"],"metadata":{"id":"inference"},"execution_count":null,"outputs":[]}]}