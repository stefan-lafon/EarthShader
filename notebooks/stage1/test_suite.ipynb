{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHGsP_fV_Hnz"
   },
   "source": [
    "**Stage 1 - Test Suite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "V1Gsr7TI_D8u"
   },
   "outputs": [],
   "source": [
    "# @title 1. Setup Environment\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 1.1 Install Dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers peft accelerate scikit-learn pandas matplotlib\n",
    "!apt-get install -y libgl1-mesa-glx xvfb > /dev/null\n",
    "!pip install -q moderngl\n",
    "\n",
    "# 1.2 Mount Drive\n",
    "from google.colab import drive\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# 1.3 Configure Paths\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n",
    "ADAPTER_PATH = os.path.join(PROJECT_ROOT, 'checkpoints/stage1_final')\n",
    "LIB_DIR = os.path.join(PROJECT_ROOT, 'lib')\n",
    "GEN_DIR = os.path.join(LIB_DIR, 'generators')\n",
    "\n",
    "# 1.4 Fix Imports (The \"Package\" Fix)\n",
    "if LIB_DIR not in sys.path:\n",
    "    sys.path.append(LIB_DIR)\n",
    "\n",
    "for folder in [LIB_DIR, GEN_DIR]:\n",
    "    init_file = os.path.join(folder, '__init__.py')\n",
    "    if not os.path.exists(init_file):\n",
    "        with open(init_file, 'w') as f: f.write(\"\")\n",
    "\n",
    "print(\"Environment Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pU6XlgDs_Rzk"
   },
   "outputs": [],
   "source": [
    "# @title 2. Load model and robust renderer\n",
    "import torch\n",
    "import moderngl\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- SHADER RENDERER ENGINE ---\n",
    "class ShaderRenderer:\n",
    "    def __init__(self, width=256, height=256):\n",
    "        self.width, self.height = width, height\n",
    "        try:\n",
    "            self.ctx = moderngl.create_context(standalone=True, backend='egl')\n",
    "        except:\n",
    "            self.ctx = moderngl.create_context(standalone=True)\n",
    "\n",
    "        self.vbo = self.ctx.buffer(np.array([-1,-1, 1,-1, -1,1, 1,1], dtype='f4'))\n",
    "        self.fbo = self.ctx.simple_framebuffer((width, height), components=3)\n",
    "        self.vert = \"#version 330\\nin vec2 in_vert; out vec2 uv; void main(){ uv=in_vert; gl_Position=vec4(in_vert,0,1); }\"\n",
    "\n",
    "    def render(self, frag_code, path):\n",
    "        full_shader = f\"#version 330\\nuniform vec2 iResolution; out vec4 f; {frag_code}\\nvoid main(){{ vec4 c; mainImage(c,gl_FragCoord.xy); f=c; }}\"\n",
    "        try:\n",
    "            prog = self.ctx.program(vertex_shader=self.vert, fragment_shader=full_shader)\n",
    "            if 'iResolution' in prog: prog['iResolution'].value = (self.width, self.height)\n",
    "            vao = self.ctx.simple_vertex_array(prog, self.vbo, 'in_vert')\n",
    "            self.fbo.use(); self.fbo.clear(); vao.render(moderngl.TRIANGLE_STRIP)\n",
    "            Image.frombytes('RGB', (self.width, self.height), self.fbo.read()).transpose(Image.FLIP_TOP_BOTTOM).save(path)\n",
    "            return True\n",
    "        except: return False\n",
    "\n",
    "renderer = ShaderRenderer(256, 256)\n",
    "\n",
    "# --- MODEL LOADING ---\n",
    "print(f\"Loading stage 1 adapter from: {ADAPTER_PATH}\")\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Load base and merge LoRA weights.\n",
    "base = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Consistent resolution is key for valid MSE metrics.\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=256*256, max_pixels=256*256)\n",
    "print(\"Model and Renderer successfully initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TxaQhx2_Xs5"
   },
   "outputs": [],
   "source": [
    "# @title 3. Run test suite and classification audit\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "from generators.primitives import generate_primitive\n",
    "\n",
    "def extract_shapes(text):\n",
    "    \"\"\"Identifies primitive names within the reasoning text.\"\"\"\n",
    "    found = []\n",
    "    # This checks the model's textual analysis for specific geometry keywords.\n",
    "    for s in ['circle', 'square', 'annulus']:\n",
    "        if s in text.lower():\n",
    "            found.append(s)\n",
    "    return sorted(list(set(found)))\n",
    "\n",
    "# Set test sample size for baseline verification.\n",
    "TEST_SAMPLES = 100\n",
    "# Define the number of detailed audit logs to capture.\n",
    "NUM_AUDIT_LOGS = 10\n",
    "\n",
    "results = []\n",
    "# Detailed records for qualitative inspection.\n",
    "audit_samples = []\n",
    "\n",
    "print(f\"Starting performance audit on {TEST_SAMPLES} unseen samples...\")\n",
    "\n",
    "for i in tqdm(range(TEST_SAMPLES)):\n",
    "    # Use seeds outside the training range to evaluate generalization.\n",
    "    seed = 5000 + i\n",
    "    gt_code, gt_analysis = generate_primitive(seed)\n",
    "\n",
    "    # Render the ground truth and handle the potential error tuple from the library.\n",
    "    render_out = renderer.render(gt_code, \"gt.png\")\n",
    "    gt_success = render_out[0] if isinstance(render_out, tuple) else render_out\n",
    "    gt_img = Image.open(\"gt.png\").convert(\"RGB\")\n",
    "\n",
    "    # Identify labels from the ground truth generator.\n",
    "    gt_shapes = extract_shapes(gt_analysis)\n",
    "\n",
    "    # Prepare the multi-modal prompt using the standard Stage 1 template.\n",
    "    conv = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}]}]\n",
    "    prompt = processor.apply_chat_template(conv, add_generation_prompt=True, tokenize=False)\n",
    "    inputs = processor(text=[prompt], images=[gt_img], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response with deterministic sampling at the training token limit.\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=768, do_sample=False)\n",
    "\n",
    "    pred_text = processor.batch_decode(out[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    # Identify labels from the model's reasoning block.\n",
    "    pred_shapes = extract_shapes(pred_text)\n",
    "\n",
    "    # Extract code and attempt a validation render.\n",
    "    pred_code = pred_text.split(\"```glsl\")[-1].split(\"```\")[0].strip() if \"```\" in pred_text else pred_text\n",
    "\n",
    "    # Capture the code and any compiler errors for the audit log.\n",
    "    render_out = renderer.render(pred_code, \"pred.png\")\n",
    "    if isinstance(render_out, tuple):\n",
    "        success, error_log = render_out\n",
    "    else:\n",
    "        success, error_log = render_out, None\n",
    "\n",
    "    # Calculate visual accuracy using Mean Squared Error.\n",
    "    mse = 1.0\n",
    "    if success:\n",
    "        p_arr = np.array(Image.open(\"pred.png\").convert(\"RGB\")).astype(float) / 255.0\n",
    "        g_arr = np.array(gt_img).astype(float) / 255.0\n",
    "        mse = np.mean((g_arr - p_arr) ** 2)\n",
    "\n",
    "    # Capture full diagnostic data for the configured constant count.\n",
    "    if i < NUM_AUDIT_LOGS:\n",
    "        audit_samples.append({\n",
    "            \"sample_index\": i,\n",
    "            \"seed\": seed,\n",
    "            \"compilation_success\": success,\n",
    "            \"compiler_error\": error_log,\n",
    "            \"ground_truth\": {\n",
    "                \"analysis\": gt_analysis,\n",
    "                \"code\": gt_code\n",
    "            },\n",
    "            \"prediction_raw\": pred_text,\n",
    "            \"mse\": mse\n",
    "        })\n",
    "\n",
    "    results.append({\n",
    "        \"id\": i,\n",
    "        \"gt_count\": len(gt_shapes),\n",
    "        \"pred_count\": len(pred_shapes),\n",
    "        \"gt_type\": gt_shapes[0] if gt_shapes else \"unknown\",\n",
    "        \"pred_type\": pred_shapes[0] if pred_shapes else \"unknown\",\n",
    "        \"compiled\": success,\n",
    "        \"mse\": mse\n",
    "    })\n",
    "\n",
    "# Save the qualitative audit results to Google Drive.\n",
    "audit_dir = os.path.join(PROJECT_ROOT, 'audits')\n",
    "os.makedirs(audit_dir, exist_ok=True)\n",
    "audit_output_path = os.path.join(audit_dir, 'stage1_audit_results.json')\n",
    "with open(audit_output_path, 'w') as f:\n",
    "    json.dump(audit_samples, f, indent=2)\n",
    "\n",
    "# Compile results into a final audit report.\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nSTAGE 1 PERFORMANCE AUDIT\\n\" + \"=\"*50)\n",
    "print(f\"1. Compilation Success:  {df['compiled'].mean():.1%}\")\n",
    "print(f\"2. Visual Precision ($MSE$): {df[df['compiled']]['mse'].mean():.4f}\")\n",
    "print(f\"Audit log saved to: {audit_output_path}\")\n",
    "\n",
    "print(\"\\n3. Complexity Accuracy (Single vs Double):\")\n",
    "print(classification_report(df['gt_count'], df['pred_count'], zero_division=0))\n",
    "\n",
    "print(\"\\n4. Shape Identity Accuracy (Circle vs Square vs Annulus):\")\n",
    "singles = df[df['gt_count'] == 1]\n",
    "print(classification_report(singles['gt_type'], singles['pred_type'], zero_division=0))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xICtFQQQQ6jt"
   },
   "outputs": [],
   "source": [
    "# @title 5. Auto-Shutdown\n",
    "# This cell will only run after the training cell finishes.\n",
    "import time\n",
    "from google.colab import runtime\n",
    "\n",
    "print(\"Training finished. Saving is complete.\")\n",
    "print(\"Shutting down runtime to save Compute Units in 60 seconds...\")\n",
    "\n",
    "# Give time for the final logs to sync to Drive\n",
    "time.sleep(60)\n",
    "\n",
    "print(\"Goodnight.\")\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOjbmyfQt81YYg528bwVgAD",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
