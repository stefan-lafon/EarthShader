{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iiV2DvBiDYx"
   },
   "source": [
    "**Stage 1 - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68693,
     "status": "ok",
     "timestamp": 1767333132739,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "PjXPuk3b2RaW",
    "outputId": "32d5960a-3db9-4756-94dc-b8054e4e06ca"
   },
   "outputs": [],
   "source": [
    "# @title 1. Setup and dependencies\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive using the stable MyDrive alias.\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# Define the project paths for the EarthShader environment.\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n",
    "DATASET_DIR = os.path.join(PROJECT_ROOT, 'dataset/stage1')\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints/stage1_adapter')\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Install the necessary libraries for Qwen2-VL support.\n",
    "print(\"Installing environment dependencies...\")\n",
    "!pip install -q git+https://github.com/huggingface/transformers peft datasets bitsandbytes accelerate qwen-vl-utils trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "387765dc02b8423da079dcdddb576634",
      "7cc62af1cc234e40a782d395ef077bdb",
      "0a89cd3ed44a402c8c7dba6d95dd8788",
      "667c0277a9394421bb1bd4b9c667929d",
      "8a962c3b47bd432db8da49486091040c",
      "4c4a467f8a5e489a9f8d5db1968f6c57",
      "9bbf407e2bc248cc86534fa165e4727a",
      "6afe3820a24046de8fe7148bf7713dac",
      "c3519fc2cd9a4f7193ad402c8c722f21",
      "a9d15bae8f3946abbfa1d7608779d69b",
      "7937de0ed5aa458c97ecb43be4b024fa",
      "a1423df9139e48189e65bb9bbe104b27",
      "fe6e297529cb42b093161c19ae924f4c",
      "121d6361f7d14df593011fa652a224da",
      "b03febd57b994689bb5e69596e51f2cb",
      "cb7cc565f0dc41a0a1bf49f636a8ccfb",
      "734da95fe81c4412bc851246a3eff846",
      "541dad9deda344a788897969be7e4d55",
      "ef1a154b755e4f0b909b4c9f3df44c50",
      "d426b3c7d90240b8b54ea7ba52e95a1b",
      "885206d5a8f24f11939cc2963f358a2a",
      "6488c46b5a40404a8770b5301cfc773b"
     ]
    },
    "executionInfo": {
     "elapsed": 137716,
     "status": "ok",
     "timestamp": 1767333270462,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "gAfjijf62X0O",
    "outputId": "b852e113-ee1b-42ff-fa28-41acfd9dd58e"
   },
   "outputs": [],
   "source": [
    "# @title 2. Model staging and local download\n",
    "import os\n",
    "import shutil\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Clean up local staging to prevent conflicts during the session.\n",
    "if os.path.exists(\"/content/qwen_local\"):\n",
    "    shutil.rmtree(\"/content/qwen_local\")\n",
    "\n",
    "# Download the base Qwen2-VL model to local runtime storage.\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "print(f\"Staging {MODEL_ID} to local runtime...\")\n",
    "\n",
    "local_model_path = snapshot_download(\n",
    "    repo_id=MODEL_ID,\n",
    "    local_dir=\"/content/qwen_local\",\n",
    "    local_dir_use_symlinks=False,\n",
    "    resume_download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "4710a2cb130b465397294e7133ee8c1b",
      "5ee084909cfc4dfc8457a50f66410130",
      "4f19107ce8ce4b899494245d849f44dd",
      "6054371203824cbdaf865d3cf429b378",
      "ac48ff74dcd04b20b9f74dce70273c92",
      "d0e8886bf6464258b17c74695e61db67",
      "cd316096bd9c48da8a77060064e33c39",
      "c6e837c44d3243a3896c1cad1e8b690d",
      "113663dd59a345888d180097c5b0ec69",
      "738f6d0508444ae2ab07c4a79cd7edf6",
      "afd637146c484169b61e6226d48216bd",
      "a61fa6cb6b8e4139b59ea9bc22b34108",
      "facaf78dbd204793a0529151d799a1c1",
      "9978f4e6a00e492685f5f0df8cebbc04",
      "a9020c462c014490a327c7d58a91fe36",
      "52add69ed03c44d3b18562afa459cbfc",
      "cd168ed9aa06425da895076b04b47c81",
      "d067b996b93c41fcacaa51bb8c83e930",
      "60c14edb8bad43b3bc0b69704458c936",
      "5df225e088a34305b45f5417d491e427",
      "f897ac7bdd3d460c8e50fbe9cc71c9e6",
      "0c7dcbd8ec614eac8ace05825bd43458",
      "0f43488a33da4417990af51b0868f8ab",
      "5fc7065778b947b2949e50453352d608",
      "a2db99db582b44fc98ad7b0b4a732230",
      "bcc5fbc79106448e92030cc4e62a2208",
      "cdbbf5abaa9e487997f87556bf23465f",
      "d53778b2288d40eead711f9b1527dfb4",
      "ccb4a5cb30fb4f1393b497c8efc75515",
      "31a2649181e6402cb17edd6184b63712",
      "bd45f25e14564b679513c4e1ea879019",
      "d2ef5563f0ea47c2a2dcc39edc5e2a89",
      "2d975632bfac474789ae736ae381f8f2",
      "6fd446c0145f4c5d9b064f175f12cf33",
      "2dc46a7bd7eb486f948b41606b19ab06",
      "3acf54943db74fb382fd1765693a2bb3",
      "1e5be57d5f584ee98ebbbbbfb0122902",
      "9a788bce8cf24043ab0ab17362ac5746",
      "a259427409fa4aa3beca6bb4c67bee75",
      "86bcd33b36244beab23254809de72c7b",
      "896aa5d2e8a74d3eb78fb03d27978123",
      "a4d2c59776e14e1495e20b292681cb92",
      "3036fc5143c142cbba62bcfd5d0336d2",
      "46cde0367c544ad3ba68011984b4d1d0",
      "895b9018e6304648bec31799b88c3dbc",
      "73538b661d6b460fa72a955ef0676d17",
      "49c88222861d403cb71a9906fbc9732c",
      "98872f15c4bf47aca9e8362a274c051f",
      "b04b298ca75b4238b43b5213a9050018",
      "4de971adc1d54bcebba900e929bd0165",
      "b0b79a476a8a402b8a8239eb0d54c8df",
      "75c6d9bd7a0a4aecb682afa46ba51e44",
      "267d555f6e6e41b99f6a90c70f9bdba6",
      "538ba91738294aaa9e962ac52975a4b5",
      "c1c6300e5e1a4dcb99119f4458376f4a",
      "8e2fa83f777742d8b938e7c2dc878363",
      "b40041327c254aa2bc05347d5c9c5927",
      "49775bca2d06408b87973c8d9253a4a2",
      "94545c8c4ad74798b5da4ffc9c7736b3",
      "c166abcef77243319b43f1c61069a691",
      "ec4ca692c2664c809f0aa070be2b38c1",
      "e3f9cbe439c3403f8b68f00e4a8ab5e0",
      "b454d795bf5c4e52a4a8d2be1a9b5499",
      "5eb6d10a918f4c25b0a633018759a9bb",
      "d7a2878bd02643a98783a1f7908afbf8",
      "a0b82b75f0454c499678cd949b574841",
      "0b1f996b74004eedaa218fccf2e5677f",
      "6246b31890ba4c86b881d6c7f796bb2c",
      "6a6f10e6f75e4b26893bb6768ece05e4",
      "de55f8e50f804d20994b1741808504c2",
      "202b1a3e142c42f082c14d9004254b40",
      "e73cc8e4d53c4a46acda67036ec1eb59",
      "29d46cca1ddb45f0b262f08240ede6c4",
      "65a8df9f168249409d90f2e419bb0730",
      "83378d3a71504c39a6e92d46bfb3d67c",
      "311e7b6367eb4562901676c99659b06e",
      "c3e792cf6e4c44dda11d045878d58758"
     ]
    },
    "executionInfo": {
     "elapsed": 114738,
     "status": "ok",
     "timestamp": 1767333385211,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "Lz9BdyrCENt1",
    "outputId": "4cf2162e-6581-4428-fad3-9fedcb865647"
   },
   "outputs": [],
   "source": [
    "# @title 3. Model loading and 4 bit quantization\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# Configure 4-bit quantization for T4 VRAM safety.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the model and processor with specific pixel constraints.\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"/content/qwen_local\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    min_pixels=256*256,\n",
    "    max_pixels=256*256\n",
    ")\n",
    "\n",
    "print(\"Model and processor successfully loaded into memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5996,
     "status": "ok",
     "timestamp": 1767333391209,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "yQg7yXo32bw5",
    "outputId": "c0c1e79c-91f1-42c7-b8f6-3938ca7b1b68"
   },
   "outputs": [],
   "source": [
    "# @title 4. Model preparation and lora setup\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# Prepare the model for kbit training sessions.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Target both the language and vision projection layers.\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Explicitly enable gradients for the vision tower parameters.\n",
    "for name, param in model.named_parameters():\n",
    "    if \"visual\" in name and (torch.is_floating_point(param) or torch.is_complex(param)):\n",
    "        param.requires_grad = True\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1767333391291,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "GyuP1QwU2e8E"
   },
   "outputs": [],
   "source": [
    "# @title 5. Scenario collation and anchoring\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# This prompt anchors the model to the EarthShader compiler API.\n",
    "SYSTEM_PROMPT = \"You are an EarthShader SDF compiler. Translate the visual primitive into a valid GLSL mainImage function using the common.glsl library.\"\n",
    "\n",
    "def final_collate_fn(batch):\n",
    "    images, full_texts, prompt_only_texts = [], [], []\n",
    "\n",
    "    for item in batch:\n",
    "        try:\n",
    "            image = Image.open(item['image_path']).convert(\"RGB\")\n",
    "        except:\n",
    "            # Create a black fallback image if the file is missing.\n",
    "            image = Image.new(\"RGB\", (256, 256), (0, 0, 0))\n",
    "        images.append(image)\n",
    "\n",
    "        prompt_conv = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n",
    "            ]}\n",
    "        ]\n",
    "\n",
    "        # Use the structured response with dense reasoning labels.\n",
    "        full_response = f\"{item['analysis']}\\n\\n```glsl\\n{item['code']}\\n```\"\n",
    "        full_conv = prompt_conv + [{\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": full_response}]}]\n",
    "\n",
    "        prompt_only_texts.append(processor.apply_chat_template(prompt_conv, tokenize=False, add_generation_prompt=True))\n",
    "        full_texts.append(processor.apply_chat_template(full_conv, tokenize=False, add_generation_prompt=False))\n",
    "\n",
    "    inputs = processor(text=full_texts, images=images, padding=\"max_length\", max_length=768, truncation=True, return_tensors=\"pt\")\n",
    "    inputs_prompts = processor(text=prompt_only_texts, images=images, padding=\"max_length\", max_length=768, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Mask the prompt tokens to focus the loss on the response.\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    for i in range(len(batch)):\n",
    "        prompt_len = inputs_prompts[\"attention_mask\"][i].sum().item()\n",
    "        labels[i, :prompt_len] = -100\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "1012e71fb4fe45079aa51fae3b4300c9",
      "75b44f9e703a4306894236069c1dfb4b",
      "e320ba5330f640d59b6cad2f9badf2bb",
      "2ff6a509f0f24b44b427c0ced8a70f09",
      "93fe74e7c5784f94b930524328c01c32",
      "5544a9962610451d9f59a06de74f4455",
      "1c7ced14e67c468ebc78d3f94ed49e13",
      "84181d7107264f3ab16256140de1226c",
      "bc5777407e664fd09cb82280f0710b26",
      "7408b48c7913406eadf714e4f0421176",
      "9c8fd0bfe0714be68dfeacf7d330dd5b"
     ]
    },
    "executionInfo": {
     "elapsed": 4402,
     "status": "ok",
     "timestamp": 1767333395691,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "Lz8w2HuLsK1n",
    "outputId": "86dadb63-e322-480a-f24e-dd8fdaf84cc3"
   },
   "outputs": [],
   "source": [
    "# @title 6. Dataset and dataloader initialization\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# 1. Load the dataset from the local registry.\n",
    "dataset_path = os.path.join(DATASET_DIR, \"dataset.jsonl\")\n",
    "raw_dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "# 2. Select exactly 2000 samples for the committed Stage 1 run.\n",
    "raw_dataset = raw_dataset.select(range(2000))\n",
    "\n",
    "# 3. Configure the data loader.\n",
    "# We set shuffle=False to ensure the run is perfectly reproducible.\n",
    "GRAD_ACCUMULATION = 4\n",
    "\n",
    "full_loader = DataLoader(\n",
    "    raw_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=final_collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset initialization complete. Committed with {len(raw_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8735556,
     "status": "ok",
     "timestamp": 1767342131254,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "AbMqkX1Uiu1k",
    "outputId": "5bb56715-adbc-41c8-f589-5606ea61fe0d"
   },
   "outputs": [],
   "source": [
    "# @title 7. Training loop and weighted loss\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "\n",
    "def compute_weighted_loss(logits, labels, tokenizer, code_weight=5.0, text_weight=0.2):\n",
    "    \"\"\"Applies higher weight to GLSL code tokens to prioritize syntax accuracy.\"\"\"\n",
    "    code_marker = tokenizer.encode(\"```glsl\", add_special_tokens=False)\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    # Standard shift for causal language modeling.\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    weights = torch.ones_like(shift_labels, dtype=torch.float32)\n",
    "\n",
    "    for i in range(shift_labels.size(0)):\n",
    "        row = shift_labels[i].tolist()\n",
    "        try:\n",
    "            # Locate where the reasoning ends and the code begins.\n",
    "            marker_idx = next(idx for idx, val in enumerate(row) if val == code_marker[-1])\n",
    "            weights[i, :marker_idx] = text_weight\n",
    "            weights[i, marker_idx:] = code_weight\n",
    "        except StopIteration:\n",
    "            # Default weighting if the marker is missing.\n",
    "            pass\n",
    "\n",
    "    flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    flat_labels = shift_labels.view(-1)\n",
    "    loss = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "    # Calculate mean loss while ignoring padding.\n",
    "    valid_mask = (flat_labels != -100)\n",
    "    weighted_loss = (loss * weights.view(-1))[valid_mask].sum() / valid_mask.sum()\n",
    "    return weighted_loss\n",
    "\n",
    "# 1. Configuration for the committed 2,000-sample run.\n",
    "GRAD_ACCUMULATION = 4\n",
    "TOTAL_ITERATIONS = 2000\n",
    "\n",
    "model.train()\n",
    "# Using PagedAdamW8bit to stay within T4 VRAM limits.\n",
    "optimizer = bnb.optim.PagedAdamW8bit(model.parameters(), lr=1e-4)\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    pbar = tqdm(full_loader, desc=\"Training stage 1\", total=TOTAL_ITERATIONS)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        # Stop exactly at the commitment limit.\n",
    "        if step >= TOTAL_ITERATIONS:\n",
    "             break\n",
    "\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = compute_weighted_loss(outputs.logits, batch[\"labels\"], processor.tokenizer)\n",
    "        (loss / GRAD_ACCUMULATION).backward()\n",
    "\n",
    "        if (step + 1) % GRAD_ACCUMULATION == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            pbar.set_postfix({'weighted_loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            # Save periodic checkpoints every 100 global steps.\n",
    "            if global_step % 100 == 0:\n",
    "                model.save_pretrained(os.path.join(CHECKPOINT_DIR, f\"step-{global_step}\"))\n",
    "\n",
    "# 2. Final save for the locked Stage 1 artifact.\n",
    "model.save_pretrained(os.path.join(PROJECT_ROOT, \"checkpoints/stage1_final\"))\n",
    "print(f\"\\n[SUCCESS] Stage 1 training complete. Final weights saved to stage1_final.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 60467,
     "status": "ok",
     "timestamp": 1767342191726,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "dpWSES8Jiz8D",
    "outputId": "c4086ddf-98ae-41db-d66d-4530ab8b7673"
   },
   "outputs": [],
   "source": [
    "# @title 8. Auto shutdown\n",
    "import time\n",
    "from google.colab import runtime\n",
    "\n",
    "# Ensure the drive has enough time to sync the final adapter files.\n",
    "print(\"Training sequence has finished. Synchronizing drive files...\")\n",
    "time.sleep(60)\n",
    "\n",
    "print(\"The system is going offline to preserve compute units.\")\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMVrwwUZJHOdPHPgdRrF/kG",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
