{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFCaX_RhBiLd"
   },
   "outputs": [],
   "source": [
    "# @title 1. Setup environment\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 1.1 Install Dependencies\n",
    "# We include the exact Stage 1 stack but ensure bitsandbytes is the latest version.\n",
    "print(\"Installing test suite dependencies...\")\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers peft accelerate scikit-learn pandas matplotlib\n",
    "!apt-get install -y libgl1-mesa-glx xvfb > /dev/null\n",
    "!pip install -q moderngl\n",
    "\n",
    "# 1.2 Mount Drive\n",
    "from google.colab import drive\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# 1.3 Configure Paths\n",
    "PROJECT_ROOT = '/content/drive/My Drive/projects/EarthShader'\n",
    "# We point to the new Stage 2 finalized adapters.\n",
    "ADAPTER_PATH = os.path.join(PROJECT_ROOT, 'checkpoints/stage2_final')\n",
    "LIB_DIR = os.path.join(PROJECT_ROOT, 'lib')\n",
    "GEN_DIR = os.path.join(LIB_DIR, 'generators')\n",
    "\n",
    "# 1.4 Fix Imports and Module Structure\n",
    "if LIB_DIR not in sys.path:\n",
    "    sys.path.append(LIB_DIR)\n",
    "\n",
    "# Ensure the library folder is recognized as a package.\n",
    "for folder in [LIB_DIR, GEN_DIR]:\n",
    "    init_file = os.path.join(folder, '__init__.py')\n",
    "    if not os.path.exists(init_file):\n",
    "        with open(init_file, 'w') as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "print(\"Environment Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iLZyF9mBuAd"
   },
   "outputs": [],
   "source": [
    "# @title 2. Load model and boolean-aware renderer\n",
    "import torch\n",
    "import moderngl\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- SHADER RENDERER ENGINE ---\n",
    "class ShaderRenderer:\n",
    "    def __init__(self, width=256, height=256):\n",
    "        self.width, self.height = width, height\n",
    "        try:\n",
    "            self.ctx = moderngl.create_context(standalone=True, backend='egl')\n",
    "        except:\n",
    "            self.ctx = moderngl.create_context(standalone=True)\n",
    "\n",
    "        self.vbo = self.ctx.buffer(np.array([-1,-1, 1,-1, -1,1, 1,1], dtype='f4'))\n",
    "        self.fbo = self.ctx.simple_framebuffer((width, height), components=3)\n",
    "        self.vert = \"#version 330\\nin vec2 in_vert; out vec2 uv; void main(){ uv=in_vert; gl_Position=vec4(in_vert,0,1); }\"\n",
    "\n",
    "    def render(self, frag_code, path):\n",
    "        # We wrap the model's mainImage function into a valid GLSL 330 program.\n",
    "        full_shader = f\"#version 330\\nuniform vec2 iResolution; out vec4 f; {frag_code}\\nvoid main(){{ vec4 c; mainImage(c,gl_FragCoord.xy); f=c; }}\"\n",
    "        try:\n",
    "            prog = self.ctx.program(vertex_shader=self.vert, fragment_shader=full_shader)\n",
    "            if 'iResolution' in prog:\n",
    "                prog['iResolution'].value = (self.width, self.height)\n",
    "            vao = self.ctx.simple_vertex_array(prog, self.vbo, 'in_vert')\n",
    "            self.fbo.use()\n",
    "            self.fbo.clear()\n",
    "            vao.render(moderngl.TRIANGLE_STRIP)\n",
    "            Image.frombytes('RGB', (self.width, self.height), self.fbo.read()).transpose(Image.FLIP_TOP_BOTTOM).save(path)\n",
    "            return True, None\n",
    "        except Exception as e:\n",
    "            return False, str(e)\n",
    "\n",
    "renderer = ShaderRenderer(256, 256)\n",
    "\n",
    "# --- MODEL LOADING ---\n",
    "print(f\"Loading Stage 2 adapters from: {ADAPTER_PATH}\")\n",
    "# Loading in 4-bit to mirror the training environment and ensure compatibility.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    min_pixels=256*256,\n",
    "    max_pixels=256*256\n",
    ")\n",
    "print(\"Model and Renderer successfully initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrdYuodSBv0L"
   },
   "outputs": [],
   "source": [
    "# @title 3. Run test suite and classification audit\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "from generators.csg import generate_csg_scenario\n",
    "\n",
    "def extract_logic_ops(text):\n",
    "    \"\"\"Identifies Boolean operations (min/max) within the reasoning or code.\"\"\"\n",
    "    ops = []\n",
    "    text_lower = text.lower()\n",
    "    # Check for union vs intersection vs subtraction logic.\n",
    "    if 'min(' in text_lower:\n",
    "        ops.append('union')\n",
    "    if 'max(' in text_lower:\n",
    "        if '-' in text_lower:\n",
    "            ops.append('subtraction')\n",
    "        else:\n",
    "            ops.append('intersection')\n",
    "    return sorted(list(set(ops)))\n",
    "\n",
    "# Audit settings.\n",
    "TEST_SAMPLES = 100\n",
    "NUM_AUDIT_LOGS = 15\n",
    "\n",
    "results = []\n",
    "audit_samples = []\n",
    "\n",
    "print(f\"Starting Stage 2 Audit on {TEST_SAMPLES} complex logic samples...\")\n",
    "\n",
    "for i in tqdm(range(TEST_SAMPLES)):\n",
    "    # Seeds 8000+ were reserved for final Stage 2 validation.\n",
    "    seed = 8000 + i\n",
    "    gt_code, gt_analysis, gt_meta = generate_csg_scenario(seed)\n",
    "\n",
    "    # Render ground truth.\n",
    "    renderer.render(gt_code, \"gt.png\")\n",
    "    gt_img = Image.open(\"gt.png\").convert(\"RGB\")\n",
    "    gt_logic = extract_logic_ops(gt_analysis)\n",
    "\n",
    "    # Prepare multimodal inference prompt.\n",
    "    conv = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}]}]\n",
    "    prompt = processor.apply_chat_template(conv, add_generation_prompt=True, tokenize=False)\n",
    "    inputs = processor(text=[prompt], images=[gt_img], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Use deterministic greedy decoding for metric reliability.\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=768, do_sample=False)\n",
    "\n",
    "    pred_text = processor.batch_decode(out[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    pred_logic = extract_logic_ops(pred_text)\n",
    "\n",
    "    # Extract code and attempt validation render.\n",
    "    pred_code = pred_text.split(\"```glsl\")[-1].split(\"```\")[0].strip() if \"```\" in pred_text else pred_text\n",
    "    success, error_log = renderer.render(pred_code, \"pred.png\")\n",
    "\n",
    "    # Calculate Mean Squared Error (Regression Check).\n",
    "    mse = 1.0\n",
    "    if success:\n",
    "        p_arr = np.array(Image.open(\"pred.png\").convert(\"RGB\")).astype(float) / 255.0\n",
    "        g_arr = np.array(gt_img).astype(float) / 255.0\n",
    "        mse = np.mean((g_arr - p_arr) ** 2)\n",
    "\n",
    "    # Capture diagnostic logs.\n",
    "    if i < NUM_AUDIT_LOGS:\n",
    "        audit_samples.append({\n",
    "            \"sample_index\": i,\n",
    "            \"seed\": seed,\n",
    "            \"compilation_success\": success,\n",
    "            \"compiler_error\": error_log,\n",
    "            \"gt_logic\": gt_logic,\n",
    "            \"pred_logic\": pred_logic,\n",
    "            \"mse\": mse,\n",
    "            \"prediction_raw\": pred_text\n",
    "        })\n",
    "\n",
    "    results.append({\n",
    "        \"id\": i,\n",
    "        \"gt_logic\": gt_logic[0] if gt_logic else \"none\",\n",
    "        \"pred_logic\": pred_logic[0] if pred_logic else \"none\",\n",
    "        \"compiled\": success,\n",
    "        \"mse\": mse\n",
    "    })\n",
    "\n",
    "# Save qualitative results to Drive.\n",
    "audit_dir = os.path.join(PROJECT_ROOT, 'audits')\n",
    "os.makedirs(audit_dir, exist_ok=True)\n",
    "audit_path = os.path.join(audit_dir, 'stage2_audit_results.json')\n",
    "with open(audit_path, 'w') as f:\n",
    "    json.dump(audit_samples, f, indent=2)\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRDgQ-ghBx_S"
   },
   "outputs": [],
   "source": [
    "# @title 4. Final performance report\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 4.1 Compile the Stage 2 logic metrics.\n",
    "# We interpret the dataframe built in Cell 3 to assess boolean proficiency.\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nSTAGE 2 PERFORMANCE AUDIT\\n\" + \"=\"*60)\n",
    "print(f\"1. Compilation Success:  {df['compiled'].mean():.1%}\")\n",
    "print(f\"2. Visual Precision (MSE): {df[df['compiled']]['mse'].mean():.4f}\")\n",
    "\n",
    "# 4.2 Logic Classification (Union vs Subtraction vs Intersection).\n",
    "# This specifically measures how well the model learned the boolean 'verbs'.\n",
    "print(\"\\n3. Logic Classification Accuracy:\")\n",
    "print(classification_report(df['gt_logic'], df['pred_logic'], zero_division=0))\n",
    "\n",
    "# 4.3 Regression Check (Baseline Comparison).\n",
    "# We compare the current MSE against the Stage 1 baseline to ensure no 'catastrophic forgetting'.\n",
    "# This uses the 0.0085 baseline established during your Stage 1 audit.\n",
    "stage1_mse_baseline = 0.0085\n",
    "current_mse = df[df['compiled']]['mse'].mean()\n",
    "\n",
    "print(\"\\n4. Regression Check (Spatial Integrity):\")\n",
    "if current_mse > (stage1_mse_baseline * 1.5):\n",
    "    print(f\"[WARNING] Potential Regression: Drift detected in spatial precision (Current MSE: {current_mse:.4f})\")\n",
    "else:\n",
    "    print(f\"[PASS] Spatial integrity maintained relative to Stage 1 baseline.\")\n",
    "\n",
    "# 4.4 Final file reference.\n",
    "audit_results_path = os.path.join(PROJECT_ROOT, 'audits/stage2_audit_results.json')\n",
    "print(f\"\\nDetailed qualitative log saved to: {audit_results_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztZQ_31NB0mu"
   },
   "outputs": [],
   "source": [
    "# @title 5. Auto-shutdown\n",
    "import time\n",
    "from google.colab import runtime\n",
    "\n",
    "print(\"Performance audit complete. Synchronization finishing...\")\n",
    "# Allow final Drive synchronization for the JSON audit log.\n",
    "time.sleep(60)\n",
    "\n",
    "print(\"Runtime disconnecting to preserve compute units.\")\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOULlKy0c6mZSNciidIS78T",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
