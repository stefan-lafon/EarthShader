{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664,
     "referenced_widgets": [
      "e7763338ba1f40738e872fd14ccc0f27",
      "98615d8caa5d45718f8c0b8acb6aaeb0",
      "d4e779499ecd4e67bbec28f289e7409e",
      "686b56846f6a44a5976e35b3e0b05b75",
      "8a3bf60c664247e596cea9751179c361",
      "5f59d98137874b0e915fcd3c86ee4a55",
      "f10e0343c71446b3ae991dc71c2d2e63",
      "eb5bd59b37fc4c2e81af74190b968e0e",
      "f3414b1d048c47fb8ddad99ef409e40c",
      "dea028fe66a34e7880397241cd8f7602",
      "2199758589694693af67c0571b9a89e9",
      "d908c1ea0c974d54809ac8d2263c9a7c",
      "286a6981fa884f109c19ab151d762ec8",
      "0f68289b1eeb4262a4bbf1e7722d7693",
      "ad12d0f5b3ba4bc0bf7bfd614cff1ff7",
      "e2a0421d5713447288d18e62f1033e10",
      "99a4f463d9884bad8735b1c09cf862cb",
      "a200156c9de6421284d005eaac3077de",
      "17ab0ab9cc26411ebe3416105cd3c328",
      "249e62b01b4240d487a9b6d10abb7f07",
      "b272643ff6754072bc21975950234396",
      "08386584a3624b4a84b99293e86fb585",
      "6c8c272e9c1c433187ee6259679cf416",
      "a9ef18ab6f804c9ba8359f57dbd10519",
      "312f0c6e2a2b44aca5d53878f8ced132",
      "9901d781106f4d8bb0ae43d8be5d61ac",
      "bbc5e03d3cea480e8b7c662a7192661d",
      "08462706ebdd4aeda9f9855373d82d29",
      "8c22d3880d4e4a89affd2b024949899f",
      "caf1c2a709364626ab2d1103392e0d0d",
      "2ddb61b6525f435fa8ebd9665f9eb8b2",
      "45b4630dcebf45828fb4f5880f68684e",
      "4ba1c912a496498c814dab0d9f49b987",
      "2877f11869054a43ac1e7a2efb251f8e",
      "a4ec58147dcd43698b4c5e2f364b0e3f",
      "960ba70df0b44ca1ba46351a26633f70",
      "4c0650fe6f1241e09f29d1e8ab7663a9",
      "ec7ab2234e114e6da56a13b81fb40eaa",
      "6aad3e768d6043dfb32ab72de7bdac39",
      "83ffbf38c42f4a208c2e87cb55cc7b9c",
      "4280ffcf7f2f4d8382f9b8d0d56cf2b3",
      "93037a8f99f2475f9973a07c62e60d2b",
      "b9a402c9ffd646acb98c30f30d2aab90",
      "a2700fd106bf4a3d9d84169878edf9ad",
      "28e6801e627f422dbe31348b7625f36b",
      "b4e0c068a75846318577fe365a4c5cc5",
      "745ee5d3da6d4eabafe1703fbe0621ad",
      "9b1b5414d0934659a5156be454c869a3",
      "709b88a4914a493089a53974e228f686",
      "a6c067feb32d4549a0d0bf28e0b447be",
      "e62c62b574c24681b46f01bb2d120d40",
      "a3c0379408c14273a6493c97bb6ed70a",
      "ef7bd3cf2151493992ee5f960e5ad010",
      "d40ff8b6addc493eb6989ead67e31408",
      "a80dd77b43de4486b5ff93e400834f47",
      "c5c78a87fbe149d3b506ad139ae08ce7",
      "403e7717053843e5862062c63cff4265",
      "41a2e3d7cf3b41749f7246c2f078c07e",
      "38d16941f23c42a49cb56d65fee93be9",
      "b547c13fa1514ea49e1c2638dddeee14",
      "d8a8710b496d49aa8f3fbf1f688d5515",
      "b8a3c1f1af0b41c0922e3268a54cb47d",
      "49376bad8faf4d84b03192e91477ab7a",
      "7b20a5be48e14e34a5b9be186fad9ca6",
      "710e097fef994625a1c78ea6118bbcd5",
      "5a5c8e7530494bddaab4e0704fae552b",
      "7c37f5f940cd450a8afbbc8784977be8",
      "1d823f15159a4d28852668e840f297bc",
      "e388a03212664bc48b32effc2603cc6b",
      "2173ead2ecec4d679db66d173cc6ec81",
      "d2931f441edf4f6a933925b66ccd7695",
      "0d5552287ef844c59a7d858c11a3ef7b",
      "d981d3961e1d47a6ac5a0477409ab1cf",
      "a2ef3b1d98be4172977a452779f33581",
      "bcf78c3078874ea7b7f8f775a8019337",
      "e6767411efc54b8b8f9f52c07318ca6c",
      "e960b71f4f334f989b13fc10390d5097",
      "220044dfe3864b5f81cf6a2d505a28a7",
      "90e8b0f6f6cd47c49b0785f44d3e4e0c",
      "b83f3939ce09446f8df03d73455218f7",
      "aab68921d1b0453da18285352b6182d6",
      "5c0bfb720d1748eabf2f91ff1d84f122",
      "94cf7a970fb84bd09bfabf2f7d85cfa9",
      "02746e185ada40f3a8b294f5b727ce89",
      "f942ca607e704c039d84e61c18e68651",
      "4038e5d929b04679915e119c6159fc25",
      "3b09e9d7f36949099f1d3db3d25a29b3",
      "14f7f685c2f248db8c9b69f3a7005805",
      "9af5cfe29a6d415c91cd1204fc116a1f",
      "5aeaf44ac47f4d0daa94a0ade43b6af2",
      "ec4fb21ab968476d9ef466ed73a6bd9a",
      "3336a3b416724e93a6cbc8e11bc61817",
      "43b70f02dca74847be177ef00b206486",
      "b4aea72b90d448d6b962c85be9388671",
      "d85e4ff129ec4052bb3059e9f3843df2",
      "2dc61bcf21254dd19593b6a4298373bf",
      "7dd16845db024a73a2de1273c38dcdf8",
      "e2585d87222443c59af0f21c6d37bf48",
      "250f3ef9af174c878def39f4a18d12a3",
      "d166973f51554d6189bc8b47f5178658",
      "b7e4cb4add8f4e98882f94e67bacb4e3",
      "a53a7df0fa79400b90d25946d8b82268",
      "8badf93890ad44239d3c60137fc6883e",
      "124c52683f584de5abd2b4d2f209cbe9",
      "d4e13b3632f3469386ac787ecf32678b",
      "e7ef78c54f434a92be3926b2f2015df0",
      "78f6e9254aef4dca8bba25e21f68efea",
      "9e25895ebc60457b9fdbe28535aa2b0b",
      "018537d2327842339d332d1b7406b920",
      "b075fb6db3594dc8ac8f50a8a940021e",
      "6426033b9428499082bfa6aeeccbc814",
      "e132a22a8a3f4d5ca7edcc27c7ae35f4",
      "1e750c49add04a158fc872e980e8772e",
      "31e4b656666a4351b5f01821d68107b4",
      "90af0dffc9f9464581b4f5bdf8f195be",
      "7b822d77baa2423d8866c148b2950f0c",
      "7ece908c03f745e4b6599d90a2e50814",
      "dd02490aa0164b6fab85b06f2678c65a",
      "9254dcf01b754180b64ee252fb7322c6",
      "4a0373f62b494b5485eeeb95130c1d1c",
      "446a82b3db9e4d28bb01fa5c2cf8cce6",
      "a32895c05cc24efabe19c8500a4dbba6",
      "9854de890b1249c4a25dd6ae4809c9d4",
      "128a0a53f96d4ebbbb93c3650611c8f5",
      "3977f1b0bf054048a3949e9bc418dfaf",
      "747964362f6e4e5abae9220f4a551205",
      "b603a0b03e6c408da3e5055762d5727f",
      "ee9d0fa3c8c6484da873e403407df4c5",
      "b5860391703945c7bf66b3c0c493535f",
      "a9dc551b6d1f472998529ba1a18fbb56",
      "c7cc47118a1e4aa2855cbea4546de698",
      "ab5a49d0e53340ebbc94eb7b1f6a408e"
     ]
    },
    "executionInfo": {
     "elapsed": 353175,
     "status": "ok",
     "timestamp": 1767460484313,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "KNJqhCgeMwQT",
    "outputId": "305ca492-0234-4ea4-cec0-4d3196b8e54b"
   },
   "outputs": [],
   "source": [
    "# @title 1. Environment and Stage 1 weight loading\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount drive.\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# 2. Project paths.\n",
    "PROJECT_ROOT = '/content/drive/My Drive/projects/EarthShader'\n",
    "STAGE1_ADAPTER_PATH = os.path.join(PROJECT_ROOT, 'checkpoints/stage1_final')\n",
    "STAGE2_OUTPUT = os.path.join(PROJECT_ROOT, 'checkpoints/stage2_final')\n",
    "DATASET_PATH = os.path.join(PROJECT_ROOT, 'dataset/stage2/dataset.jsonl')\n",
    "\n",
    "os.makedirs(STAGE2_OUTPUT, exist_ok=True)\n",
    "\n",
    "# 3. Forced installation of latest bitsandbytes for 4-bit support.\n",
    "print(\"Ensuring environment dependencies...\")\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q git+https://github.com/huggingface/transformers peft datasets accelerate qwen-vl-utils trl\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# 4. Memory-optimized quantization config.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Use float16 for T4 compatibility.\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "print(f\"Loading base architecture: {BASE_MODEL_ID}\")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}, #\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 5. Inject Stage 1 weights as trainable adapters.\n",
    "print(f\"Loading Stage 1 foundation: {STAGE1_ADAPTER_PATH}\")\n",
    "model = PeftModel.from_pretrained(model, STAGE1_ADAPTER_PATH, is_trainable=True)\n",
    "\n",
    "# 6. VRAM OPTIMIZATION: Freeze vision tower.\n",
    "# We only need to train the language/logic layers for CSG.\n",
    "for name, param in model.named_parameters():\n",
    "    if \"visual\" in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    min_pixels=256*256,\n",
    "    max_pixels=256*256\n",
    ")\n",
    "\n",
    "print(\"[SUCCESS] Model loaded and vision tower frozen to save VRAM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6a3d6bb77b5a42179a8c8c4e67e9bcdf",
      "ed81941335084cad90c07995044c2329",
      "b67a81fb7e3944d1bfa769429064c59c",
      "bf5282377faa456cadfa21d55c37d239",
      "406d11642785405d865ac041fd98b9fe",
      "a2bc573b55d740e7a934ce2a2f6c7e83",
      "894400b5d83543dfaa627d8c6c2bba58",
      "48a2ff25b1134e5698dd3270d4e3c718",
      "8180b3dbe7454915be104229f2eeb6fc",
      "bc41191215f9487a96815a654a7698d9",
      "b30e6efb6d384b609b4cdf24fc6afe5f"
     ]
    },
    "executionInfo": {
     "elapsed": 3691,
     "status": "ok",
     "timestamp": 1767460488009,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "obrbRDPJNDlb",
    "outputId": "05dd5f59-1f51-4961-eee5-8791500ce904"
   },
   "outputs": [],
   "source": [
    "# @title 2. Scenario collation and anchoring\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SYSTEM_PROMPT = \"You are an EarthShader SDF compiler. Translate the visual primitive into a valid GLSL mainImage function using the common.glsl library.\"\n",
    "\n",
    "def final_collate_fn(batch):\n",
    "    images, full_texts, prompt_only_texts = [], [], []\n",
    "    for item in batch:\n",
    "        try:\n",
    "            image = Image.open(item['image_path']).convert(\"RGB\")\n",
    "        except:\n",
    "            raise FileNotFoundError(f\"Missing training image: {item['image_path']}. Check your mount!\") from e\n",
    "        images.append(image)\n",
    "\n",
    "        prompt_conv = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n",
    "            ]}\n",
    "        ]\n",
    "\n",
    "        full_response = f\"{item['analysis']}\\n\\n```glsl\\n{item['code']}\\n```\"\n",
    "        full_conv = prompt_conv + [{\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": full_response}]}]\n",
    "\n",
    "        prompt_only_texts.append(processor.apply_chat_template(prompt_conv, tokenize=False, add_generation_prompt=True))\n",
    "        full_texts.append(processor.apply_chat_template(full_conv, tokenize=False, add_generation_prompt=False))\n",
    "\n",
    "    inputs = processor(text=full_texts, images=images, padding=\"max_length\", max_length=768, truncation=True, return_tensors=\"pt\")\n",
    "    inputs_prompts = processor(text=prompt_only_texts, images=images, padding=\"max_length\", max_length=768, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    for i in range(len(batch)):\n",
    "        prompt_len = inputs_prompts[\"attention_mask\"][i].sum().item()\n",
    "        labels[i, :prompt_len] = -100\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "raw_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "full_loader = DataLoader(raw_dataset, batch_size=1, shuffle=False, collate_fn=final_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 19725361,
     "status": "ok",
     "timestamp": 1767480213386,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "0fNH4kVCNFHT",
    "outputId": "c3651f48-f5b5-480f-a743-2ff89db9d6a1"
   },
   "outputs": [],
   "source": [
    "# @title 3. Training loop and weighted loss stabilization\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import time\n",
    "import os\n",
    "from google.colab import runtime\n",
    "\n",
    "def compute_stage2_loss(logits, labels, tokenizer, code_weight=3.0, csg_weight=5.0, text_weight=0.5):\n",
    "    \"\"\"Calculates loss using float32 to prevent numerical overflow with csg weights.\"\"\"\n",
    "    # Encode target tokens for logical weighting.\n",
    "    csg_tokens = tokenizer.encode(\"min max\", add_special_tokens=False)\n",
    "    code_marker = tokenizer.encode(\"```glsl\", add_special_tokens=False)\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    # Prepare shifts for causal language modeling.\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Cast to float32 for stable weighted loss accumulation.\n",
    "    weights = torch.ones_like(shift_labels, dtype=torch.float32)\n",
    "\n",
    "    for i in range(shift_labels.size(0)):\n",
    "        row = shift_labels[i].tolist()\n",
    "        try:\n",
    "            # Locate the start of the code block.\n",
    "            marker_idx = next(idx for idx, val in enumerate(row) if val == code_marker[-1])\n",
    "            weights[i, :marker_idx] = text_weight\n",
    "            weights[i, marker_idx:] = code_weight\n",
    "            for idx in range(marker_idx, len(row)):\n",
    "                if row[idx] in csg_tokens:\n",
    "                    weights[i, idx] = csg_weight\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    # Move logits to float32 before the cross entropy calculation.\n",
    "    flat_logits = shift_logits.view(-1, shift_logits.size(-1)).float()\n",
    "    flat_labels = shift_labels.view(-1)\n",
    "\n",
    "    loss = loss_fct(flat_logits, flat_labels)\n",
    "    valid_mask = (flat_labels != -100)\n",
    "\n",
    "    # Apply weights and normalize by the number of valid tokens.\n",
    "    weighted_loss = (loss * weights.view(-1))[valid_mask].sum() / valid_mask.sum()\n",
    "    return weighted_loss\n",
    "\n",
    "# 1. Hyperparameters for stabilized Stage 2.\n",
    "GRAD_ACCUMULATION = 4\n",
    "TOTAL_ITERATIONS = 2000\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-5 # Lowered to prevent divergence.\n",
    "TOTAL_STEPS = (TOTAL_ITERATIONS // GRAD_ACCUMULATION) * EPOCHS\n",
    "\n",
    "model.train()\n",
    "optimizer = bnb.optim.PagedAdamW8bit(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(TOTAL_STEPS * 0.1),\n",
    "    num_training_steps=TOTAL_STEPS\n",
    ")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(full_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", total=TOTAL_ITERATIONS)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        if step >= TOTAL_ITERATIONS:\n",
    "            break\n",
    "\n",
    "        # Periodic memory cleanup for T4 stability.\n",
    "        if step % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # Calculate loss in high precision.\n",
    "        loss = compute_stage2_loss(outputs.logits, batch[\"labels\"], processor.tokenizer)\n",
    "\n",
    "        # Skip training if loss explodes despite precautions.\n",
    "        if torch.isnan(loss) or loss.item() > 100.0:\n",
    "            print(f\"Skipping unstable loss at step {step}.\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "        (loss / GRAD_ACCUMULATION).backward()\n",
    "\n",
    "        if (step + 1) % GRAD_ACCUMULATION == 0:\n",
    "            # Tighter gradient clipping to stabilize Boolean logic learnin.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "\n",
    "            # Save checkpoints every 100 steps to protect progress.\n",
    "            if global_step % 100 == 0:\n",
    "                model.save_pretrained(os.path.join(STAGE2_OUTPUT, f\"step-{global_step}\"))\n",
    "\n",
    "# 2. Final save for the refined Stage 2 weights.\n",
    "model.save_pretrained(STAGE2_OUTPUT)\n",
    "print(f\"\\n[SUCCESS] Stabilized Stage 2 training complete.\")\n",
    "\n",
    "# 3. Shutdown sequence to save credits.\n",
    "time.sleep(60)\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "EibP6umWOB1a"
   },
   "outputs": [],
   "source": [
    "# @title 4. Auto shutdown\n",
    "import time\n",
    "from google.colab import runtime\n",
    "\n",
    "# Ensure the drive has enough time to sync the final adapter files.\n",
    "print(\"Training sequence has finished. Synchronizing drive files...\")\n",
    "time.sleep(60)\n",
    "\n",
    "print(\"The system is going offline to preserve compute units.\")\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOH+smYI6XlIcbojdR0hWPj",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
