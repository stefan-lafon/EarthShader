{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJPK45sduVul"
   },
   "source": [
    "**Stage 1 - Data Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xAV9t0Xhf7ye"
   },
   "outputs": [],
   "source": [
    "# @title 1. Setup & Dependencies\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "try:\n",
    "    import moderngl\n",
    "except ImportError:\n",
    "    subprocess.check_call(['apt-get', 'install', '-y', 'libgl1-mesa-glx'])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"moderngl\"])\n",
    "    import moderngl\n",
    "\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from google.colab import drive\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "LIB_DIR = os.path.join(PROJECT_ROOT, 'lib')\n",
    "\n",
    "# Ensure we can import from lib\n",
    "if LIB_DIR not in sys.path:\n",
    "    sys.path.insert(0, LIB_DIR)\n",
    "\n",
    "# CONFIG\n",
    "DATASET_ROOT = os.path.join(PROJECT_ROOT, 'dataset/stage1')\n",
    "IMAGES_DIR = os.path.join(DATASET_ROOT, 'images')\n",
    "JSONL_PATH = os.path.join(DATASET_ROOT, 'dataset.jsonl')\n",
    "NUM_SAMPLES = 1500\n",
    "IMG_SIZE = 512\n",
    "\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "print(f\"Setup complete. Dataset will be saved to: {DATASET_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GpuQPXD-aSXN"
   },
   "outputs": [],
   "source": [
    "# @title 2. Import Generator Logic\n",
    "try:\n",
    "    # Try standard package import\n",
    "    from lib.generators.primitives import generate_primitive\n",
    "    from lib.gl_renderer import ShaderRenderer\n",
    "    print(\"Library loaded successfully via standard import.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Standard import failed ({e}). Trying direct file load...\")\n",
    "\n",
    "    # Direct loading if Colab pathing is fighting us\n",
    "    import importlib.util\n",
    "\n",
    "    # Load Renderer\n",
    "    spec_r = importlib.util.spec_from_file_location(\"gl_renderer\", os.path.join(LIB_DIR, \"gl_renderer.py\"))\n",
    "    mod_r = importlib.util.module_from_spec(spec_r)\n",
    "    spec_r.loader.exec_module(mod_r)\n",
    "    ShaderRenderer = mod_r.ShaderRenderer\n",
    "\n",
    "    # Load Base\n",
    "    spec_b = importlib.util.spec_from_file_location(\"base\", os.path.join(LIB_DIR, \"generators/base.py\"))\n",
    "    mod_b = importlib.util.module_from_spec(spec_b)\n",
    "    spec_b.loader.exec_module(mod_b)\n",
    "\n",
    "    # Load Primitives (injecting base dependency manually)\n",
    "    spec_p = importlib.util.spec_from_file_location(\"primitives\", os.path.join(LIB_DIR, \"generators/primitives.py\"))\n",
    "    mod_p = importlib.util.module_from_spec(spec_p)\n",
    "    mod_p.base = mod_b # Mock the relative import\n",
    "    spec_p.loader.exec_module(mod_p)\n",
    "\n",
    "    generate_primitive = mod_p.generate_primitive\n",
    "    print(\"Direct file load successful.\")\n",
    "\n",
    "# Quick test to verify generator works\n",
    "print(\"\\n--- Testing Generator ---\")\n",
    "test_code, test_analysis = generate_primitive(0)\n",
    "print(f\"Generator test passed. Sample analysis:\\n{test_analysis[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "u7iphw6raWWs"
   },
   "outputs": [],
   "source": [
    "# @title 3. Generation Loop (Canonical Mode)\n",
    "\n",
    "# CONFIG\n",
    "FORCE_REGEN = True  # Set to True to wipe existing data and start over\n",
    "\n",
    "if 'renderer' not in locals():\n",
    "    renderer = ShaderRenderer(width=IMG_SIZE, height=IMG_SIZE)\n",
    "    print(\"Renderer initialized\")\n",
    "\n",
    "# Resume Logic\n",
    "dataset_entries = []\n",
    "failed_count = 0\n",
    "\n",
    "# Track statistics\n",
    "stats = {\n",
    "    'single_circle': 0,\n",
    "    'single_square': 0,\n",
    "    'single_ring': 0,\n",
    "    'composition': 0\n",
    "}\n",
    "\n",
    "if FORCE_REGEN:\n",
    "    print(\"[FORCE REGEN] Wiping existing registry...\")\n",
    "    with open(JSONL_PATH, 'w') as f:\n",
    "        pass # Create empty file\n",
    "    existing_count = 0\n",
    "else:\n",
    "    if os.path.exists(JSONL_PATH):\n",
    "        with open(JSONL_PATH, 'r') as f:\n",
    "            existing_count = sum(1 for line in f)\n",
    "        print(f\"Found {existing_count} existing samples.\")\n",
    "    else:\n",
    "        with open(JSONL_PATH, 'w') as f:\n",
    "            pass\n",
    "        existing_count = 0\n",
    "\n",
    "if existing_count < NUM_SAMPLES:\n",
    "    print(f\"Generating samples {existing_count} to {NUM_SAMPLES}...\")\n",
    "    print(f\"Target: {NUM_SAMPLES} samples with Canonical Syntax + Diverse Analysis\\n\")\n",
    "\n",
    "    pbar = tqdm(range(existing_count, NUM_SAMPLES), desc=\"Generating\")\n",
    "\n",
    "    for i in pbar:\n",
    "        try:\n",
    "            # CALL THE LIBRARY\n",
    "            code, analysis = generate_primitive(i)\n",
    "\n",
    "            filename = f\"stage1_{i:05d}.png\"\n",
    "            filepath = os.path.join(IMAGES_DIR, filename)\n",
    "\n",
    "            # Render logic\n",
    "            success = renderer.render(code, filepath)\n",
    "\n",
    "            if success:\n",
    "                # Update Stats\n",
    "                if 'Composition' in analysis:\n",
    "                    stats['composition'] += 1\n",
    "                elif 'Circle' in analysis or 'Radial' in analysis:\n",
    "                    stats['single_circle'] += 1\n",
    "                elif 'Square' in analysis or 'Box' in analysis:\n",
    "                    stats['single_square'] += 1\n",
    "                elif 'Ring' in analysis or 'Annulus' in analysis:\n",
    "                    stats['single_ring'] += 1\n",
    "\n",
    "                entry = {\n",
    "                    \"image_path\": filepath,\n",
    "                    \"analysis\": analysis,\n",
    "                    \"code\": code\n",
    "                }\n",
    "                dataset_entries.append(entry)\n",
    "\n",
    "                # Incremental Save (Every 100 for performance)\n",
    "                if len(dataset_entries) >= 100:\n",
    "                    with open(JSONL_PATH, 'a') as f:\n",
    "                        for e in dataset_entries:\n",
    "                            f.write(json.dumps(e) + '\\n')\n",
    "                    dataset_entries = []\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({\n",
    "                        'fails': failed_count,\n",
    "                        'dbl': stats['composition']\n",
    "                    })\n",
    "            else:\n",
    "                # Handle Render Failure\n",
    "                failed_count += 1\n",
    "                pbar.set_description(f\"Gen ({failed_count} fails)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[CRITICAL ERROR] Sample {i}: {e}\")\n",
    "            failed_count += 1\n",
    "\n",
    "    # Final Flush\n",
    "    if dataset_entries:\n",
    "        with open(JSONL_PATH, 'a') as f:\n",
    "            for e in dataset_entries:\n",
    "                f.write(json.dumps(e) + '\\n')\n",
    "\n",
    "    # Print final statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATION COMPLETE (Canonical Mode)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Samples: {NUM_SAMPLES - failed_count}\")\n",
    "    print(f\"Failed: {failed_count} ({100*failed_count/NUM_SAMPLES:.1f}%)\")\n",
    "    print(f\"\\nDistribution:\")\n",
    "    print(f\"   Single Circle: {stats['single_circle']:>5}\")\n",
    "    print(f\"   Single Square: {stats['single_square']:>5}\")\n",
    "    print(f\"   Single Ring:   {stats['single_ring']:>5}\")\n",
    "    print(f\"   Compositions:  {stats['composition']:>5}\")\n",
    "else:\n",
    "    print(\"Dataset already complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fwbRcstdW8ak"
   },
   "outputs": [],
   "source": [
    "# @title 4. Validator (Strict Canonical Check)\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "def validate_canonical_data():\n",
    "    if not os.path.exists(JSONL_PATH):\n",
    "        print(\"Dataset not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Analyzing dataset structure...\\n\")\n",
    "    with open(JSONL_PATH, 'r') as f:\n",
    "        entries = [json.loads(line) for line in f]\n",
    "\n",
    "    total = len(entries)\n",
    "\n",
    "    unique_code_logic = set()\n",
    "    unique_analyses = set()\n",
    "\n",
    "    for entry in entries:\n",
    "        full_code = entry['code']\n",
    "\n",
    "        # 1. Extract Logic Only (Remove C-style comments)\n",
    "        # We use regex to strip the /* ANALYSIS ... */ block\n",
    "        glsl_logic = re.sub(r'/\\*.*?\\*/', '', full_code, flags=re.DOTALL)\n",
    "\n",
    "        # Normalize: Remove numbers, whitespace, and symbols to check PURE structure\n",
    "        # This collapses \"vec2 v = vec2(0.1, 0.2)\" and \"vec2 v = vec2(0.9, 0.9)\" to the same string\n",
    "        normalized_logic = ''.join(c for c in glsl_logic if c.isalpha())\n",
    "        unique_code_logic.add(normalized_logic)\n",
    "\n",
    "        # 2. Extract Analysis Only\n",
    "        # We grab the text inside the comment block to check for diversity there\n",
    "        analysis_match = re.search(r'/\\* ANALYSIS\\n(.*?)\\n\\*/', full_code, flags=re.DOTALL)\n",
    "        if analysis_match:\n",
    "            unique_analyses.add(analysis_match.group(1).strip())\n",
    "\n",
    "    print(f\"Metrics:\")\n",
    "    print(f\"   Unique Logic Structures: {len(unique_code_logic)}\")\n",
    "    print(f\"   Unique Analysis Blocks:  {len(unique_analyses)}\")\n",
    "\n",
    "    # VALIDATION LOGIC\n",
    "    print(\"\\nReport:\")\n",
    "\n",
    "    # We expect VERY LOW code variance (ideally < 10 for the 3 primitives + 3 compositions)\n",
    "    if len(unique_code_logic) <= 15:\n",
    "        print(\"   [PASS] Code Structure is Canonical (Low Variance)\")\n",
    "    else:\n",
    "        print(f\"   [FAIL] Too much code variation ({len(unique_code_logic)} variants). Check primitives.py.\")\n",
    "\n",
    "    # We expect HIGH analysis variance\n",
    "    if len(unique_analyses) > 20:\n",
    "        print(\"   [PASS] Analysis Reasoning is Diverse\")\n",
    "    else:\n",
    "        print(\"   [FAIL] Analysis is repetitive. Model will overfit.\")\n",
    "\n",
    "validate_canonical_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "v8TqUQAagX6o"
   },
   "outputs": [],
   "source": [
    "# @title 5. Inspector (Random Sample)\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "import base64\n",
    "\n",
    "def inspect_random_sample():\n",
    "    if not os.path.exists(JSONL_PATH):\n",
    "        print(f\"Waiting for data... {JSONL_PATH} not found yet.\")\n",
    "        return\n",
    "\n",
    "    # Load lines to pick a random one\n",
    "    with open(JSONL_PATH, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if not lines:\n",
    "        print(\"Dataset is empty.\")\n",
    "        return\n",
    "\n",
    "    # Pick random entry\n",
    "    line = random.choice(lines)\n",
    "    entry = json.loads(line)\n",
    "\n",
    "    img_path = entry['image_path']\n",
    "    code = entry['code']\n",
    "    analysis = entry['analysis']\n",
    "\n",
    "    # Detect variant type for label\n",
    "    variant_label = \"Unknown\"\n",
    "    if 'inline' in analysis.lower():\n",
    "        variant_label = \"Inline\"\n",
    "    elif 'verbose' in analysis.lower():\n",
    "        variant_label = \"Verbose\"\n",
    "    elif 'intermediate' in analysis.lower():\n",
    "        variant_label = \"Intermediate\"\n",
    "    elif 'alternative' in analysis.lower():\n",
    "        variant_label = \"Alternative\"\n",
    "\n",
    "    if 'composition' in analysis.lower() or 'two shapes' in analysis.lower():\n",
    "        variant_label += \" + Composition\"\n",
    "\n",
    "    # Display using HTML (side-by-side layout)\n",
    "    if os.path.exists(img_path):\n",
    "        # Encode image to base64 for HTML display\n",
    "        with open(img_path, 'rb') as f:\n",
    "            img_data = base64.b64encode(f.read()).decode()\n",
    "\n",
    "        # Escape code for HTML\n",
    "        code_html = code.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n",
    "\n",
    "        html = f\"\"\"\n",
    "        <div style=\"display: flex; gap: 20px; font-family: monospace;\">\n",
    "            <div style=\"flex: 1; background: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto;\">\n",
    "                <h3 style=\"margin-top: 0;\">Generated GLSL [{variant_label}]</h3>\n",
    "                <pre style=\"white-space: pre-wrap; font-size: 11px;\">{code_html}</pre>\n",
    "            </div>\n",
    "            <div style=\"flex: 1;\">\n",
    "                <h3 style=\"margin-top: 0;\">Rendered Output</h3>\n",
    "                <img src=\"data:image/png;base64,{img_data}\" style=\"max-width: 100%; border: 1px solid #ccc;\">\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "        display(HTML(html))\n",
    "\n",
    "        # Print analysis separately\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS BLOCK:\")\n",
    "        print(\"=\"*60)\n",
    "        print(analysis)\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(f\"Image missing at path: {img_path}\")\n",
    "\n",
    "# Run inspector\n",
    "inspect_random_sample()\n",
    "\n",
    "# Run multiple times to see variety\n",
    "print(\"\\nTIP: Run this cell multiple times to verify diversity across samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2fZnxKvwXBn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/content/drive/MyDrive/projects/EarthShader/dataset/stage1/dataset.jsonl', 'r') as f:\n",
    "    has_artifacts = False\n",
    "    for i, line in enumerate(f):\n",
    "        entry = json.loads(line)\n",
    "        code = entry['code']\n",
    "\n",
    "        # Check for floating point artifacts\n",
    "        if '00000000' in code:\n",
    "            print(f\"Sample {i} HAS ARTIFACTS:\")\n",
    "            print(code[:500])\n",
    "            has_artifacts = True\n",
    "            break\n",
    "\n",
    "    if not has_artifacts:\n",
    "        print(\"SUCCESS: No floating point artifacts found in dataset!\")\n",
    "        print(\"\\nSample code from first entry:\")\n",
    "        f.seek(0)\n",
    "        first = json.loads(f.readline())\n",
    "        print(first['code'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMau1bARanrWqJnXQQsPeky",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
