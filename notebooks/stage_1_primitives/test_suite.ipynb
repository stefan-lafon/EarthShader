{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHGsP_fV_Hnz"
   },
   "source": [
    "**Stage 1 - Test Suite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120123,
     "status": "ok",
     "timestamp": 1765671317026,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "V1Gsr7TI_D8u",
    "outputId": "4450dfb3-551e-412f-a893-6a2cb16e08f7"
   },
   "outputs": [],
   "source": [
    "# @title 1. Setup Environment\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 1.1 Install Dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers peft accelerate scikit-learn pandas matplotlib\n",
    "!apt-get install -y libgl1-mesa-glx xvfb > /dev/null\n",
    "!pip install -q moderngl\n",
    "\n",
    "# 1.2 Mount Drive\n",
    "from google.colab import drive\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# 1.3 Configure Paths\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n",
    "ADAPTER_PATH = os.path.join(PROJECT_ROOT, 'checkpoints/stage1_final')\n",
    "LIB_DIR = os.path.join(PROJECT_ROOT, 'lib')\n",
    "GEN_DIR = os.path.join(LIB_DIR, 'generators')\n",
    "\n",
    "# 1.4 Fix Imports (The \"Package\" Fix)\n",
    "if LIB_DIR not in sys.path:\n",
    "    sys.path.append(LIB_DIR)\n",
    "\n",
    "for folder in [LIB_DIR, GEN_DIR]:\n",
    "    init_file = os.path.join(folder, '__init__.py')\n",
    "    if not os.path.exists(init_file):\n",
    "        with open(init_file, 'w') as f: f.write(\"\")\n",
    "\n",
    "print(\"Environment Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261,
     "referenced_widgets": [
      "5f1e4cdf56f846e48e7781465ec98d64",
      "88372d850e4042b69f9870d17cdf8e9f",
      "46c877be62104f958da877ad967b90cc",
      "93010b70082f464fac1d55eeb6568ece",
      "0c2b100274034852b9707e6058fe00a3",
      "7d156af5356b4aa0be01f1826b0e72dd",
      "1fdfde0ed77948a481cf582f4535b899",
      "3046933f40dc4ef19555886081d0d03c",
      "8107b5ec83914d4da6908932e5a94534",
      "b14b556bfe6841ed9b83ead3b925760f",
      "a3418b2f0cd2472e902bc232707a7c85",
      "f43729cb5e714445a45ff720c1552b46",
      "5d71aa4b301f426e8235ed3612d8df3a",
      "4f375fca8fb443fd8b55b92575ec4092",
      "deeb0ec175574c2fb5bb5b7dcfea9d09",
      "0493750ea35e4dbaa6c54cc53f8a4555",
      "272c3fd586324eeabb4314ac825c3964",
      "ae42d90ed3e140388a2ad35fc63c0dbe",
      "e8a8b4d435d2410d810c213eebb9929a",
      "a1bce19544394839946ce7eae11554fa",
      "b6f8dcebab0a4e8cb59e89fa84632a4b",
      "09e7d442b93d4ddb8f57b46c433735f6",
      "b7ec9276ac8240bfab1edd9fcde2ff24",
      "badd1a8561cf49db9e3a60e0752e373f",
      "a3a7850d6a7342b89e205e3d1a5e79d6",
      "e25cd70d7d4f4418aa2c510c29154ba9",
      "c08cf7d502824be9bc64bb1ea0fcf1d8",
      "e84d1812410b40b08dfe4b63396d271d",
      "4e172022873a456586e1a9c003ec5a21",
      "daa466117e23410197dd9024be35a084",
      "46bdce8f66a4486dbf2bf71fd7e14534",
      "7ae12e8353f7414eae12d68aa540cdc8",
      "bceae0d34541470795dd270347b555c2",
      "447a67f4cdc645cbb9492f98b3bbbee3",
      "da5629aa0d4c4aa18c7d6203f73a6c12",
      "52cfcef6a0d44807873e5cab5eea6447",
      "a210375241684d0cac677db9a1949d12",
      "97afe19d057f4af6b76f81c604243c32",
      "f23b0cf72ad54079aacfde4f0b025e9d",
      "439be8804cfb4d3f890799a4a19bc21f",
      "df007beb92f14e98b0d55f7b85a1846c",
      "b0649509f5a84e2bb06167e482c94ee3",
      "b3567b0a72af4e92860fba2c957d4b94",
      "393871f9d28645a68168e91b6e2c16c2",
      "59a2b8db7b2c40de90ca8d137682f9e3",
      "abde65faddb94cf1aba57c79cd20da0b",
      "a5d2cc4d33c4454d8e21ab3e82d3200e",
      "ea341084ba4c4c8b95b85409e79d9d0e",
      "c61dfffdb3df49bf9bb9ff22772da053",
      "1405c268334c487694e855cc2c90ac62",
      "a026b268fee04845ac17b3ffa0d335a8",
      "e7b2d84e04bc4ebd90d3de78f3c72ae1",
      "62f3e82223c14b6991d299c0279bebc8",
      "d13076e6a0d944979d5f005645ad786e",
      "3ae1f9f9794747d5a17777253b62bd38",
      "1e6115b74cfa420291d558b033b78d78",
      "4a10fcb638ac49dd81fc78f2184f08f2",
      "050987f743734794b690e52aadd9b3ab",
      "b38f5de35bbe491cba23505689881925",
      "e0fb78c707864da99de16272cb22eda1",
      "2fdfe3146a6e49fb9b614d64e66c8908",
      "5377a446b41b41b09f1f756b1360f788",
      "5b93219dca6a4bcdb3c628c322806d04",
      "43351dd91a504a04b46bb9a13c42a50e",
      "f010f3e39bcf462fa4d268a8672fc23b",
      "f79ae01b770a43a89151216292c38a5d"
     ]
    },
    "executionInfo": {
     "elapsed": 303594,
     "status": "ok",
     "timestamp": 1765671620631,
     "user": {
      "displayName": "Stefan Lafon",
      "userId": "17149650627927548318"
     },
     "user_tz": 480
    },
    "id": "pU6XlgDs_Rzk",
    "outputId": "451fd906-8eb0-4d15-be8b-f75d7a621988"
   },
   "outputs": [],
   "source": [
    "# @title 2. Load Model & Robust Renderer\n",
    "import torch\n",
    "import moderngl\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import importlib.util\n",
    "\n",
    "# --- ROBUST RENDERER (Embedded to prevent import issues) ---\n",
    "class ShaderRenderer:\n",
    "    def __init__(self, width=256, height=256):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.ctx = None\n",
    "        try:\n",
    "            self.ctx = moderngl.create_context(standalone=True, backend='egl')\n",
    "        except Exception:\n",
    "            try:\n",
    "                self.ctx = moderngl.create_context(standalone=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Renderer Init Error: {e}\")\n",
    "\n",
    "        # Geometry\n",
    "        vertices = np.array([-1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0], dtype='f4')\n",
    "        if self.ctx:\n",
    "            self.vbo = self.ctx.buffer(vertices.tobytes())\n",
    "            self.fbo = self.ctx.simple_framebuffer((self.width, self.height), components=3)\n",
    "            self.vert_shader = '''\n",
    "            #version 330\n",
    "            in vec2 in_vert;\n",
    "            out vec2 uv;\n",
    "            void main() {\n",
    "                uv = in_vert;\n",
    "                gl_Position = vec4(in_vert, 0.0, 1.0);\n",
    "            }\n",
    "            '''\n",
    "\n",
    "            # Load Common Lib\n",
    "            common_path = os.path.join(LIB_DIR, 'common.glsl')\n",
    "            self.common_lib = \"\"\n",
    "            if os.path.exists(common_path):\n",
    "                with open(common_path, 'r') as f:\n",
    "                    self.common_lib = f.read()\n",
    "\n",
    "    def render(self, fragment_code, output_path):\n",
    "        if not self.ctx: return False\n",
    "\n",
    "        full_frag_shader = f'''\n",
    "        #version 330\n",
    "        uniform vec2 iResolution;\n",
    "        out vec4 fragColor;\n",
    "        {self.common_lib}\n",
    "        {fragment_code}\n",
    "        void main() {{\n",
    "            vec4 color;\n",
    "            mainImage(color, gl_FragCoord.xy);\n",
    "            fragColor = color;\n",
    "        }}\n",
    "        '''\n",
    "        try:\n",
    "            prog = self.ctx.program(vertex_shader=self.vert_shader, fragment_shader=full_frag_shader)\n",
    "            if 'iResolution' in prog: prog['iResolution'].value = (self.width, self.height)\n",
    "            vao = self.ctx.simple_vertex_array(prog, self.vbo, 'in_vert')\n",
    "\n",
    "            self.fbo.use()\n",
    "            self.fbo.clear(0.0, 0.0, 0.0, 1.0)\n",
    "            vao.render(moderngl.TRIANGLE_STRIP)\n",
    "\n",
    "            data = self.fbo.read(components=3)\n",
    "            image = Image.frombytes('RGB', self.fbo.size, data).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            image.save(output_path)\n",
    "\n",
    "            vao.release()\n",
    "            prog.release()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "# Initialize\n",
    "renderer = ShaderRenderer(width=256, height=256)\n",
    "\n",
    "# Load Generators\n",
    "try:\n",
    "    if 'generators' in sys.modules: del sys.modules['generators']\n",
    "    from generators.primitives import generate_primitive\n",
    "    print(\"Generators loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Generator Import Error: {e}\")\n",
    "\n",
    "# Load Model\n",
    "print(f\"Loading Adapter from: {ADAPTER_PATH}\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "processor = Qwen2VLProcessor.from_pretrained(ADAPTER_PATH, min_pixels=256*256, max_pixels=256*256)\n",
    "print(\"Model Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TxaQhx2_Xs5",
    "outputId": "31920036-44b8-4fef-bbbd-2d887cd11d19"
   },
   "outputs": [],
   "source": [
    "# @title 3. Run Test Suite (100 Samples)\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CONFIG\n",
    "TEST_SAMPLES = 100\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Helpers\n",
    "def parse_shape_name(text):\n",
    "    # Try getting explicit comment first\n",
    "    match = re.search(r\"// (?:Shape|Geometry):\\s*(\\w+)\", text, re.IGNORECASE)\n",
    "    if match: return match.group(1).lower()\n",
    "\n",
    "    # Fallback: Primitive analysis\n",
    "    text = text.lower()\n",
    "    if \"circle\" in text and \"length\" in text: return \"circle\"\n",
    "    if \"square\" in text and \"max\" in text: return \"square\"\n",
    "    if \"triangle\" in text: return \"triangle\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def extract_code(text):\n",
    "    if \"```glsl\" in text: return text.split(\"```glsl\")[1].split(\"```\")[0].strip()\n",
    "    if \"```\" in text: parts = text.split(\"```\"); return parts[1].strip() if len(parts)>1 else text\n",
    "    if \"void mainImage\" in text: return text[text.find(\"void mainImage\"):].strip()\n",
    "    return text\n",
    "\n",
    "def calculate_mse(img1, img2):\n",
    "    arr1 = np.array(img1.resize((256,256))).astype(float) / 255.0\n",
    "    arr2 = np.array(img2.resize((256,256))).astype(float) / 255.0\n",
    "    return np.mean((arr1 - arr2) ** 2)\n",
    "\n",
    "# --- PHASE 1: GENERATE DATA ---\n",
    "print(f\"Generating {TEST_SAMPLES} ground truth samples...\")\n",
    "gt_data = []\n",
    "for i in range(TEST_SAMPLES):\n",
    "    code, analysis = generate_primitive(i)\n",
    "    shape = parse_shape_name(analysis) # Get label from GT analysis\n",
    "\n",
    "    renderer.render(code, \"temp_gt.png\")\n",
    "    img = Image.open(\"temp_gt.png\").convert(\"RGB\")\n",
    "\n",
    "    gt_data.append({\"gt_code\": code, \"gt_shape\": shape, \"gt_image\": img})\n",
    "\n",
    "# --- PHASE 2: INFERENCE ---\n",
    "print(\"Running Inference...\")\n",
    "predictions = []\n",
    "batch_prompts = []\n",
    "batch_images = []\n",
    "\n",
    "for i in tqdm(range(0, TEST_SAMPLES, BATCH_SIZE)):\n",
    "    batch = gt_data[i : i + BATCH_SIZE]\n",
    "\n",
    "    # Prepare batch\n",
    "    imgs = [item[\"gt_image\"] for item in batch]\n",
    "    prompts = []\n",
    "    for img in imgs:\n",
    "        prompts.append([\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n",
    "            ]}\n",
    "        ])\n",
    "\n",
    "    # Tokenize\n",
    "    text_inputs = processor.apply_chat_template(prompts, add_generation_prompt=True, tokenize=False)\n",
    "    inputs = processor(text=text_inputs, images=imgs, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate (Strict Deterministic)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False  # Crucial for code\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_ids = output_ids[:, inputs.input_ids.shape[1]:]\n",
    "    output_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    predictions.extend(output_texts)\n",
    "\n",
    "# --- PHASE 3: EVALUATE ---\n",
    "print(\"Evaluating...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "visual_errors = []\n",
    "compile_success = 0\n",
    "\n",
    "for i, text in enumerate(predictions):\n",
    "    gt = gt_data[i]\n",
    "\n",
    "    pred_code = extract_code(text)\n",
    "    pred_shape = parse_shape_name(text) # Parse model's own logic\n",
    "\n",
    "    # Render Prediction\n",
    "    success = renderer.render(pred_code, \"temp_pred.png\")\n",
    "\n",
    "    mse = 1.0\n",
    "    if success and os.path.exists(\"temp_pred.png\"):\n",
    "        compile_success += 1\n",
    "        pred_img = Image.open(\"temp_pred.png\").convert(\"RGB\")\n",
    "        mse = calculate_mse(gt[\"gt_image\"], pred_img)\n",
    "\n",
    "    y_true.append(gt[\"gt_shape\"])\n",
    "    y_pred.append(pred_shape)\n",
    "    visual_errors.append(mse)\n",
    "\n",
    "# --- REPORT ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"TEST REPORT (N={TEST_SAMPLES})\")\n",
    "print(\"=\"*40)\n",
    "print(f\"1. Compilation Success: {compile_success}/{TEST_SAMPLES} ({compile_success/TEST_SAMPLES:.1%})\")\n",
    "print(f\"2. Average Visual Error (MSE): {np.mean(visual_errors):.4f} (Lower is better)\")\n",
    "\n",
    "print(\"\\n3. Classification Report (Did it identify the shape?):\")\n",
    "print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "print(\"\\n4. Confusion Matrix:\")\n",
    "labels = sorted(list(set(y_true + y_pred)))\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "print(pd.DataFrame(cm, index=labels, columns=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "background_save": true
    },
    "id": "xICtFQQQQ6jt"
   },
   "outputs": [],
   "source": [
    "# @title 4. Auto-Shutdown\n",
    "# This cell will only run after the training cell finishes.\n",
    "import time\n",
    "from google.colab import runtime\n",
    "\n",
    "print(\"Training finished. Saving is complete.\")\n",
    "print(\"Shutting down runtime to save Compute Units in 60 seconds...\")\n",
    "\n",
    "# Give time for the final logs to sync to Drive\n",
    "time.sleep(60)\n",
    "\n",
    "print(\"Goodnight.\")\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN+ecvkmQ0Y3wq5c1OQvO3J",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
