{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOpL0+kxjM3PmuPFv/+6TFd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Stage 1 - Test Suite**"],"metadata":{"id":"dHGsP_fV_Hnz"}},{"cell_type":"code","source":["# @title 1. Setup Environment\n","import os\n","import sys\n","import subprocess\n","\n","# 1.1 Install Dependencies\n","print(\"Installing dependencies...\")\n","!pip install -q -U bitsandbytes\n","!pip install -q -U git+https://github.com/huggingface/transformers peft accelerate scikit-learn pandas\n","!apt-get install -y libgl1-mesa-glx > /dev/null\n","!pip install -q moderngl\n","\n","# 1.2 Mount Drive\n","from google.colab import drive\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","# 1.3 Configure Paths\n","PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n","ADAPTER_PATH = os.path.join(PROJECT_ROOT, 'checkpoints/stage1_final')\n","LIB_DIR = os.path.join(PROJECT_ROOT, 'lib')\n","\n","# 1.4 Register Library\n","if PROJECT_ROOT not in sys.path:\n","    sys.path.append(PROJECT_ROOT)\n","if LIB_DIR not in sys.path:\n","    sys.path.insert(0, LIB_DIR)\n","\n","print(\"Environment Ready.\")"],"metadata":{"cellView":"form","id":"V1Gsr7TI_D8u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 2. Load Model & Generators\n","import torch\n","import importlib.util\n","from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n","from peft import PeftModel\n","\n","# 2.1 Load Generator Library\n","try:\n","    from generators.primitives import generate_primitive\n","    from gl_renderer import ShaderRenderer\n","    print(\"Libraries loaded successfully.\")\n","except ImportError:\n","    # Direct load fallback for Colab path issues\n","    print(\"Standard import failed. Using direct file injection...\")\n","\n","    # Load Renderer\n","    spec_r = importlib.util.spec_from_file_location(\"gl_renderer\", os.path.join(LIB_DIR, \"gl_renderer.py\"))\n","    mod_r = importlib.util.module_from_spec(spec_r)\n","    spec_r.loader.exec_module(mod_r)\n","    ShaderRenderer = mod_r.ShaderRenderer\n","\n","    # Load Base Generator\n","    spec_b = importlib.util.spec_from_file_location(\"base\", os.path.join(LIB_DIR, \"generators/base.py\"))\n","    mod_b = importlib.util.module_from_spec(spec_b)\n","    spec_b.loader.exec_module(mod_b)\n","\n","    # Load Primitives Generator\n","    spec_p = importlib.util.spec_from_file_location(\"primitives\", os.path.join(LIB_DIR, \"generators/primitives.py\"))\n","    mod_p = importlib.util.module_from_spec(spec_p)\n","    mod_p.base = mod_b\n","    spec_p.loader.exec_module(mod_p)\n","    generate_primitive = mod_p.generate_primitive\n","\n","# 2.2 Initialize Renderer\n","renderer = ShaderRenderer(width=256, height=256)\n","\n","# 2.3 Load Model\n","print(f\"Loading Adapter from: {ADAPTER_PATH}\")\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    \"Qwen/Qwen2-VL-7B-Instruct\",\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16,\n",")\n","\n","model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n","processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n","print(\"Model Ready.\")"],"metadata":{"id":"pU6XlgDs_Rzk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3. Run Test Suite\n","import re\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# CONFIG\n","TEST_SAMPLES = 100  # Set to 100 for a quick 3-minute test, 1000 takes ~40 mins\n","BATCH_SIZE = 4\n","\n","# Helpers\n","def parse_shape_name(text):\n","    # Regex handles optional spaces: \"// Shape : Circle\"\n","    match = re.search(r\"// Shape:\\s*(\\w+)\", text)\n","    return match.group(1).lower() if match else \"unknown\"\n","\n","def extract_code(text):\n","    if \"// GLSL CODE\" in text:\n","        return text.split(\"// GLSL CODE\")[1].strip()\n","    if \"```glsl\" in text:\n","        return text.split(\"```glsl\")[1].split(\"```\")[0].strip()\n","    return text\n","\n","def calculate_mse(img1, img2):\n","    # Normalize to 0-1 range for a standard MSE\n","    arr1 = np.array(img1).astype(float) / 255.0\n","    arr2 = np.array(img2).astype(float) / 255.0\n","    return np.mean((arr1 - arr2) ** 2)\n","\n","# --- PHASE 1: GENERATE GROUND TRUTH ---\n","print(f\"Generating {TEST_SAMPLES} ground truth samples using library...\")\n","gt_data = []\n","\n","for i in range(TEST_SAMPLES):\n","    # Use the library to create a valid test case\n","    code, analysis = generate_primitive(i)\n","    shape = parse_shape_name(analysis)\n","\n","    renderer.render(code, \"temp_gt.png\")\n","    img = Image.open(\"temp_gt.png\").convert(\"RGB\")\n","\n","    gt_data.append({\n","        \"gt_code\": code,\n","        \"gt_shape\": shape,\n","        \"gt_image\": img\n","    })\n","\n","# --- PHASE 2: RUN INFERENCE ---\n","print(\"Running Inference...\")\n","predictions = []\n","\n","for i in tqdm(range(0, TEST_SAMPLES, BATCH_SIZE)):\n","    batch = gt_data[i : i + BATCH_SIZE]\n","    batch_images = [item[\"gt_image\"] for item in batch]\n","\n","    # Prepare Prompt\n","    prompts = []\n","    for img in batch_images:\n","        prompts.append([\n","            {\"role\": \"user\", \"content\": [\n","                {\"type\": \"image\", \"image\": img},\n","                {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n","            ]}\n","        ])\n","\n","    text_inputs = [processor.apply_chat_template(p, add_generation_prompt=True) for p in prompts]\n","\n","    inputs = processor(\n","        text=text_inputs,\n","        images=batch_images,\n","        padding=True,\n","        return_tensors=\"pt\"\n","    ).to(model.device)\n","\n","    with torch.no_grad():\n","        output_ids = model.generate(**inputs, max_new_tokens=512)\n","\n","    generated_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, output_ids)]\n","    output_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n","\n","    predictions.extend(output_texts)\n","\n","# --- PHASE 3: EVALUATE ---\n","print(\"Evaluating Results...\")\n","y_true = []\n","y_pred = []\n","visual_errors = []\n","compile_success = 0\n","\n","for i, text in enumerate(predictions):\n","    gt = gt_data[i]\n","\n","    # 1. Parse Output\n","    pred_code = extract_code(text)\n","    pred_shape = parse_shape_name(text)\n","\n","    # 2. Compile & Render\n","    success = renderer.render(pred_code, \"temp_pred.png\")\n","\n","    # 3. Calculate Metrics\n","    mse = 1.0 # Default high error penalty\n","    if success and os.path.exists(\"temp_pred.png\"):\n","        compile_success += 1\n","        pred_img = Image.open(\"temp_pred.png\").convert(\"RGB\")\n","        mse = calculate_mse(gt[\"gt_image\"], pred_img)\n","\n","    y_true.append(gt[\"gt_shape\"])\n","    y_pred.append(pred_shape)\n","    visual_errors.append(mse)\n","\n","# --- REPORT ---\n","print(\"\\n\" + \"=\"*40)\n","print(f\"TEST REPORT (N={TEST_SAMPLES})\")\n","print(\"=\"*40)\n","\n","print(f\"\\n1. Compilation Success: {compile_success}/{TEST_SAMPLES} ({compile_success/TEST_SAMPLES:.1%})\")\n","print(f\"2. Average Visual Error (MSE): {np.mean(visual_errors):.4f}\")\n","\n","print(\"\\n3. Classification Report:\")\n","print(classification_report(y_true, y_pred, zero_division=0))\n","\n","print(\"\\n4. Confusion Matrix:\")\n","# Include all labels found in both truth and predictions (handles hallucinations)\n","all_labels = sorted(list(set(y_true + y_pred)))\n","cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n","print(pd.DataFrame(cm, index=all_labels, columns=all_labels))"],"metadata":{"cellView":"form","id":"7TxaQhx2_Xs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 4. Auto-Shutdown\n","# This cell will only run after the training cell finishes.\n","import time\n","from google.colab import runtime\n","\n","print(\"Training finished. Saving is complete.\")\n","print(\"Shutting down runtime to save Compute Units in 60 seconds...\")\n","\n","# Give time for the final logs to sync to Drive\n","time.sleep(60)\n","\n","print(\"Goodnight.\")\n","runtime.unassign()"],"metadata":{"cellView":"form","id":"xICtFQQQQ6jt"},"execution_count":null,"outputs":[]}]}