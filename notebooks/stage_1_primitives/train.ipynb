{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP25TXT6lVa0sybJ+hof87Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"ZgGK61a5kFLV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Stage 1 - Training**"],"metadata":{"id":"2iiV2DvBiDYx"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PjXPuk3b2RaW"},"outputs":[],"source":["# @title 1. Setup & Dependencies\n","import os\n","import sys\n","import subprocess\n","\n","# 1.1 Mount Drive\n","from google.colab import drive\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","# 1.2 Project Paths\n","PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n","DATASET_DIR = os.path.join(PROJECT_ROOT, 'dataset/stage1')\n","CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints/stage1_adapter')\n","LOG_DIR = os.path.join(PROJECT_ROOT, 'logs')\n","\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","os.makedirs(LOG_DIR, exist_ok=True)\n","\n","# 1.3 Install Standard HF Libraries\n","# We need the latest transformers for Qwen2-VL support\n","print(\"Installing Hugging Face Libraries...\")\n","packages = [\n","    \"git+https://github.com/huggingface/transformers\",\n","    \"peft\",\n","    \"datasets\",\n","    \"bitsandbytes\",\n","    \"accelerate\",\n","    \"qwen-vl-utils\",\n","    \"trl\"\n","]\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages)\n","\n","print(\"Environment Ready.\")"]},{"cell_type":"code","source":["# @title 2. Download Model (Stable Version)\n","import os\n","import shutil\n","from huggingface_hub import snapshot_download\n","\n","# 1. Clean up the broken 2.5 download to free up space\n","if os.path.exists(\"/content/qwen_local\"):\n","    print(\"Cleaning up incompatible model files...\")\n","    shutil.rmtree(\"/content/qwen_local\")\n","\n","# 2. Download the Stable Qwen2-VL (Not 2.5)\n","# This version is fully compatible with the current transformers library\n","MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n","\n","print(f\"Downloading {MODEL_ID}...\")\n","local_model_path = snapshot_download(\n","    repo_id=MODEL_ID,\n","    local_dir=\"/content/qwen_local\",\n","    local_dir_use_symlinks=False,\n","    resume_download=True\n",")\n","\n","print(f\"Model ready at: {local_model_path}\")"],"metadata":{"id":"gAfjijf62X0O","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3. Load Model (Corrected & Standardized)\n","import torch\n","from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","MODEL_PATH = \"/content/qwen_local\"\n","\n","# --- RESOLUTION SETTINGS ---\n","# We calculate total pixels based on target resolution (Width x Height)\n","# 1. Min: 224x224 (Standard minimal size for ViTs)\n","MIN_PIXELS = 224 * 224\n","\n","# 2. Max: 256x256 (Aggressive cap to save VRAM on T4)\n","MAX_PIXELS = 256 * 256\n","\n","print(f\"Loading Processor with:\")\n","print(f\" - Min Resolution: 224x224 ({MIN_PIXELS} pixels)\")\n","print(f\" - Max Resolution: 256x256 ({MAX_PIXELS} pixels)\")\n","\n","processor = Qwen2VLProcessor.from_pretrained(\n","    MODEL_PATH,\n","    min_pixels=MIN_PIXELS,\n","    max_pixels=MAX_PIXELS\n",")\n","\n","# 2. Load Base Model (4-bit)\n","BNB_CONFIG = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    MODEL_PATH,\n","    quantization_config=BNB_CONFIG,\n","    device_map={\"\": 0},\n","    torch_dtype=torch.float16,\n","    low_cpu_mem_usage=True,\n",")\n","\n","# 3. Apply LoRA\n","model.gradient_checkpointing_enable()\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.CAUSAL_LM,\n","    modules_to_save=[],\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()\n","model.enable_input_require_grads()"],"metadata":{"id":"Lz9BdyrCENt1","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 4. Dataset & DataLoader\n","import json\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","\n","# 1. Define Paths (Self-Contained)\n","PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n","DATASET_DIR = os.path.join(PROJECT_ROOT, 'dataset/stage1')\n","\n","# 2. Define Dataset Class\n","class ShaderDataset(Dataset):\n","    def __init__(self, jsonl_path):\n","        self.samples = []\n","        if not os.path.exists(jsonl_path):\n","            print(f\"Error: {jsonl_path} not found.\")\n","            return\n","\n","        with open(jsonl_path, 'r') as f:\n","            for line in f:\n","                try:\n","                    entry = json.loads(line)\n","                    if os.path.exists(entry['image_path']):\n","                        self.samples.append(entry)\n","                except:\n","                    continue\n","        print(f\"Loaded {len(self.samples)} valid samples.\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        return self.samples[idx]\n","\n","# 3. Define Collate Function\n","def collate_fn(batch):\n","    images = []\n","    texts = []\n","\n","    for item in batch:\n","        # Load Image on the fly to save RAM\n","        try:\n","            image = Image.open(item['image_path']).convert(\"RGB\")\n","        except:\n","            image = Image.new(\"RGB\", (256, 256), (0, 0, 0)) # Fallback\n","\n","        images.append(image)\n","\n","        # Construct ChatML Prompt\n","        analysis = item['analysis']\n","        code = item['code']\n","        full_response = f\"{analysis}\\n\\n// GLSL CODE\\n{code}\"\n","\n","        # Standard Qwen2-VL Prompt Format\n","        conversation = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"image\"},\n","                    {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n","                ]\n","            },\n","            {\n","                \"role\": \"assistant\",\n","                \"content\": [{\"type\": \"text\", \"text\": full_response}]\n","            }\n","        ]\n","\n","        # Apply template using the processor\n","        text_prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n","        texts.append(text_prompt)\n","\n","    # Process Batch\n","    inputs = processor(\n","        text=texts,\n","        images=images,\n","        padding=True,\n","        return_tensors=\"pt\",\n","    )\n","\n","    # Create Labels\n","    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n","\n","    # Mask padding\n","    if processor.tokenizer.pad_token_id is not None:\n","        inputs[\"labels\"][inputs[\"input_ids\"] == processor.tokenizer.pad_token_id] = -100\n","\n","    return inputs\n","\n","# 4. Initialize Loader\n","jsonl_file = os.path.join(DATASET_DIR, 'dataset.jsonl')\n","dataset = ShaderDataset(jsonl_file)\n","\n","train_dataloader = DataLoader(\n","    dataset,\n","    batch_size=1,       # T4 Limit\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    num_workers=2,      # Pre-load images in background\n","    pin_memory=True\n",")\n","\n","print(\"DataLoader ready.\")"],"metadata":{"id":"yQg7yXo32bw5","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 5. Speed Test (Fixed Masking & LR)\n","from torch.optim import AdamW\n","import bitsandbytes as bnb\n","from tqdm import tqdm\n","import torch\n","import gc\n","\n","# 1. Shrink Dataset (Overfit Test)\n","mini_dataset = torch.utils.data.Subset(dataset, range(128))\n","\n","# 2. Collator with LABEL MASKING (Critical Fix)\n","def masked_collate_fn(batch):\n","    # Standard processing first\n","    inputs = collate_fn(batch)\n","\n","    # Now mask the user prompts so we don't train on them\n","    # Qwen uses <|im_start|>assistant as the trigger\n","    # We find the index of the answer start and mask everything before it\n","    input_ids = inputs[\"input_ids\"]\n","    labels = inputs[\"labels\"]\n","\n","    # Qwen token IDs (approximate, scanning for standard chat template structure)\n","    # A robust way is to just mask the first N tokens if length is consistent,\n","    # but dynamic masking is safer.\n","    for i in range(len(input_ids)):\n","        # Find the start of the assistant's response\n","        # This is a heuristic: finding the last instance of the User/System sequence\n","        # For simplicity in this test: Mask the first 50 tokens (The visual + prompt header)\n","        # Real Qwen prompt is roughly 40-60 tokens long + image tokens\n","        labels[i, :64] = -100 # Ignore the image and the question\n","\n","    return inputs\n","\n","# 3. Loader\n","mini_loader = DataLoader(\n","    mini_dataset,\n","    batch_size=1,\n","    shuffle=True,\n","    collate_fn=masked_collate_fn, # Use new masker\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# 4. Configuration\n","EPOCHS = 10             # More epochs to force convergence\n","GRAD_ACCUMULATION = 8\n","LEARNING_RATE = 5e-5    # Lower LR for stability\n","\n","# 5. Optimizer Reset\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = bnb.optim.PagedAdamW8bit(params, lr=LEARNING_RATE)\n","\n","# 6. Loop\n","model.train()\n","print(f\"ðŸš€ Starting MASKED Sanity Check...\")\n","print(\"Target Loss: < 1.0\")\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","for epoch in range(EPOCHS):\n","    progress_bar = tqdm(mini_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n","    total_loss = 0\n","    steps = 0\n","\n","    for step, batch in enumerate(progress_bar):\n","        try:\n","            batch = {k: v.to(model.device) for k, v in batch.items()}\n","\n","            outputs = model(**batch, use_cache=False)\n","            loss = outputs.loss / GRAD_ACCUMULATION\n","            loss.backward()\n","\n","            total_loss += outputs.loss.item()\n","            steps += 1\n","\n","            if (step + 1) % GRAD_ACCUMULATION == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                avg_loss = total_loss / steps\n","                progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})\n","\n","        except RuntimeError as e:\n","            if \"out of memory\" in str(e):\n","                optimizer.zero_grad()\n","                torch.cuda.empty_cache()\n","            else:\n","                raise e"],"metadata":{"cellView":"form","id":"xRkK9SmAXIuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 5. Full Training Run\n","from torch.optim import AdamW\n","import bitsandbytes as bnb\n","from tqdm import tqdm\n","import torch\n","import gc\n","import os\n","from PIL import Image\n","\n","# 1. Configuration\n","EPOCHS = 1\n","GRAD_ACCUMULATION = 8\n","LEARNING_RATE = 2e-4\n","SAVE_STEPS = 500\n","MAX_LENGTH = 512\n","\n","# 2. Define Collator (With Text Truncation)\n","def smart_collate_fn(batch):\n","    images = []\n","    full_texts = []\n","    prompt_only_texts = []\n","\n","    for item in batch:\n","        # Load Image\n","        try:\n","            image = Image.open(item['image_path']).convert(\"RGB\")\n","        except:\n","            image = Image.new(\"RGB\", (256, 256), (0, 0, 0))\n","        images.append(image)\n","\n","        # Prepare Texts\n","        analysis = item['analysis']\n","        code = item['code']\n","        full_response = f\"{analysis}\\n\\n// GLSL CODE\\n{code}\"\n","\n","        # Prompt Only\n","        conversation_prompt = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"image\"},\n","                    {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n","                ]\n","            }\n","        ]\n","        prompt_str = processor.apply_chat_template(conversation_prompt, tokenize=False, add_generation_prompt=True)\n","\n","        # Full Conversation\n","        conversation_full = [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"image\"},\n","                    {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n","                ]\n","            },\n","            {\n","                \"role\": \"assistant\",\n","                \"content\": [{\"type\": \"text\", \"text\": full_response}]\n","            }\n","        ]\n","        full_str = processor.apply_chat_template(conversation_full, tokenize=False, add_generation_prompt=False)\n","\n","        prompt_only_texts.append(prompt_str)\n","        full_texts.append(full_str)\n","\n","    # Tokenize with HARD CAPS\n","    inputs = processor(\n","        text=full_texts,\n","        images=images,\n","        padding=\"max_length\", # Consistent tensor size\n","        max_length=MAX_LENGTH, # Cap at 512\n","        truncation=True,       # Cut off if longer\n","        return_tensors=\"pt\",\n","    )\n","\n","    # We need a separate pass for prompts to calculate masking length\n","    # We use the same caps to ensure indices match\n","    inputs_prompts = processor(\n","        text=prompt_only_texts,\n","        images=images,\n","        padding=\"max_length\",\n","        max_length=MAX_LENGTH,\n","        truncation=True,\n","        return_tensors=\"pt\",\n","    )\n","\n","    # Masking\n","    labels = inputs[\"input_ids\"].clone()\n","    for i in range(len(batch)):\n","        # Calculate prompt length\n","        prompt_len = inputs_prompts[\"attention_mask\"][i].sum().item()\n","\n","        # Safety clamp\n","        prompt_len = min(prompt_len, MAX_LENGTH)\n","\n","        # Mask prompt\n","        labels[i, :prompt_len] = -100\n","\n","        # Mask padding (tokens equal to pad_token_id)\n","        if processor.tokenizer.pad_token_id is not None:\n","            labels[i][inputs[\"input_ids\"][i] == processor.tokenizer.pad_token_id] = -100\n","\n","    inputs[\"labels\"] = labels\n","    return inputs\n","\n","# 3. Create Full Loader\n","full_loader = DataLoader(\n","    dataset,\n","    batch_size=1,\n","    shuffle=True,\n","    collate_fn=smart_collate_fn,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# 4. Optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = bnb.optim.PagedAdamW8bit(params, lr=LEARNING_RATE)\n","\n","# 5. Resume Logic\n","global_step = 0\n","if os.path.exists(CHECKPOINT_DIR):\n","    checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"checkpoint\")]\n","    if checkpoints:\n","        checkpoints.sort(key=lambda x: int(x.split('-')[-1]))\n","        print(f\"  Resuming from epoch 0 (clean start recommended for Stage 1)\")\n","\n","# 6. Training Loop\n","model.train()\n","print(f\"  Starting FULL TRAINING (Max Len={MAX_LENGTH})\")\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","for epoch in range(EPOCHS):\n","    progress_bar = tqdm(full_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n","    total_loss = 0\n","    steps = 0\n","\n","    for step, batch in enumerate(progress_bar):\n","        try:\n","            # Move to GPU\n","            batch = {k: v.to(model.device) for k, v in batch.items()}\n","\n","            # Forward\n","            outputs = model(**batch, use_cache=False)\n","            loss = outputs.loss / GRAD_ACCUMULATION\n","            loss.backward()\n","\n","            total_loss += outputs.loss.item()\n","            steps += 1\n","\n","            # Update Step\n","            if (step + 1) % GRAD_ACCUMULATION == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                global_step += 1\n","\n","                # Logging\n","                avg_loss = total_loss / steps\n","                progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})\n","\n","                # Saving\n","                if global_step % SAVE_STEPS == 0:\n","                    save_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint-{global_step}\")\n","                    print(f\"\\nSaving checkpoint: {save_path}\")\n","                    model.save_pretrained(save_path)\n","                    processor.save_pretrained(save_path)\n","\n","                    # Cleanup\n","                    gc.collect()\n","                    torch.cuda.empty_cache()\n","\n","        except RuntimeError as e:\n","            if \"out of memory\" in str(e):\n","                print(f\"OOM at step {step}. Recovering...\")\n","                optimizer.zero_grad()\n","                torch.cuda.empty_cache()\n","            else:\n","                raise e\n","\n","# 7. Final Save\n","final_path = os.path.join(PROJECT_ROOT, \"checkpoints/stage1_final\")\n","print(f\"\\nSaving FINAL model to {final_path}\")\n","model.save_pretrained(final_path)\n","processor.save_pretrained(final_path)\n","print(\"Training Complete.\")"],"metadata":{"id":"GyuP1QwU2e8E","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 6. Auto-Shutdown\n","# This cell will only run after the training cell finishes.\n","import time\n","from google.colab import runtime\n","\n","print(\"Training finished. Saving is complete.\")\n","print(\"Shutting down runtime to save Compute Units in 60 seconds...\")\n","\n","# Give time for the final logs to sync to Drive\n","time.sleep(60)\n","\n","print(\"Goodnight.\")\n","runtime.unassign()"],"metadata":{"id":"Lz8w2HuLsK1n"},"execution_count":null,"outputs":[]}]}