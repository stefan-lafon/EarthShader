{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iiV2DvBiDYx"
   },
   "source": [
    "**Stage 1 - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjXPuk3b2RaW"
   },
   "outputs": [],
   "source": [
    "# @title 1. Setup & Dependencies\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 1.1 Mount Drive\n",
    "from google.colab import drive\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# 1.2 Project Paths\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n",
    "DATASET_DIR = os.path.join(PROJECT_ROOT, 'dataset/stage1')\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints/stage1_adapter')\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, 'logs')\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# 1.3 Install Standard HF Libraries\n",
    "# We need the latest transformers for Qwen2-VL support\n",
    "print(\"Installing Hugging Face Libraries...\")\n",
    "packages = [\n",
    "    \"git+https://github.com/huggingface/transformers\",\n",
    "    \"peft\",\n",
    "    \"datasets\",\n",
    "    \"bitsandbytes\",\n",
    "    \"accelerate\",\n",
    "    \"qwen-vl-utils\",\n",
    "    \"trl\"\n",
    "]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages)\n",
    "\n",
    "print(\"Environment Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gAfjijf62X0O"
   },
   "outputs": [],
   "source": [
    "# @title 2. Download Model (Stable Version)\n",
    "import os\n",
    "import shutil\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# 1. Clean up the broken 2.5 download to free up space\n",
    "if os.path.exists(\"/content/qwen_local\"):\n",
    "    print(\"Cleaning up incompatible model files...\")\n",
    "    shutil.rmtree(\"/content/qwen_local\")\n",
    "\n",
    "# 2. Download the Stable Qwen2-VL (Not 2.5)\n",
    "# This version is fully compatible with the current transformers library\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "print(f\"Downloading {MODEL_ID}...\")\n",
    "local_model_path = snapshot_download(\n",
    "    repo_id=MODEL_ID,\n",
    "    local_dir=\"/content/qwen_local\",\n",
    "    local_dir_use_symlinks=False,\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "print(f\"Model ready at: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Lz9BdyrCENt1"
   },
   "outputs": [],
   "source": [
    "# @title 3. Load Model\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "MODEL_PATH = \"/content/qwen_local\"\n",
    "\n",
    "# --- RESOLUTION SETTINGS ---\n",
    "MIN_PIXELS = 224 * 224\n",
    "MAX_PIXELS = 256 * 256\n",
    "\n",
    "print(f\"Loading Processor with:\")\n",
    "print(f\" - Min Resolution: 224x224 ({MIN_PIXELS} pixels)\")\n",
    "print(f\" - Max Resolution: 256x256 ({MAX_PIXELS} pixels)\")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS\n",
    ")\n",
    "\n",
    "# 2. Load Base Model (4-bit)\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# 3. Apply LoRA (VISION + LANGUAGE)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    # Language model projections\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # Attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"           # FFN\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    modules_to_save=[],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# CRITICAL: Manually add LoRA to vision tower\n",
    "# Qwen2-VL uses a ViT-based visual encoder\n",
    "print(\"\\nAdding LoRA adapters to vision tower...\")\n",
    "from peft import inject_adapter_in_model\n",
    "\n",
    "# Find vision tower attention layers\n",
    "vision_target_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    if 'visual' in name and ('q_proj' in name or 'k_proj' in name or 'v_proj' in name or 'out_proj' in name):\n",
    "        vision_target_modules.append(name)\n",
    "\n",
    "print(f\"Found {len(vision_target_modules)} vision attention layers\")\n",
    "\n",
    "# Apply LoRA config to vision modules\n",
    "vision_lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],  # ViT attention\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Inject adapters into vision tower\n",
    "for name, module in model.named_modules():\n",
    "    if 'visual' in name and any(target in name for target in ['q_proj', 'k_proj', 'v_proj', 'out_proj']):\n",
    "        # Mark as trainable\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(\"\\nVision tower unfrozen and LoRA adapters added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yQg7yXo32bw5"
   },
   "outputs": [],
   "source": [
    "# @title 4. Dataset & DataLoader\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Define Paths (Self-Contained)\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/projects/EarthShader'\n",
    "DATASET_DIR = os.path.join(PROJECT_ROOT, 'dataset/stage1')\n",
    "\n",
    "# 2. Define Dataset Class\n",
    "class ShaderDataset(Dataset):\n",
    "    def __init__(self, jsonl_path):\n",
    "        self.samples = []\n",
    "        if not os.path.exists(jsonl_path):\n",
    "            print(f\"Error: {jsonl_path} not found.\")\n",
    "            return\n",
    "\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    if os.path.exists(entry['image_path']):\n",
    "                        self.samples.append(entry)\n",
    "                except:\n",
    "                    continue\n",
    "        print(f\"Loaded {len(self.samples)} valid samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# 3. Define Collate Function\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    texts = []\n",
    "\n",
    "    for item in batch:\n",
    "        # Load Image on the fly to save RAM\n",
    "        try:\n",
    "            image = Image.open(item['image_path']).convert(\"RGB\")\n",
    "        except:\n",
    "            image = Image.new(\"RGB\", (256, 256), (0, 0, 0)) # Fallback\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "        # FIX: 'code' already contains the Analysis header from primitives.py\n",
    "        # We use it directly to avoid duplication.\n",
    "        full_response = item['code']\n",
    "\n",
    "        # Standard Qwen2-VL Prompt Format\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": full_response}]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Apply template using the processor\n",
    "        text_prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text_prompt)\n",
    "\n",
    "    # Process Batch\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Create Labels\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask padding\n",
    "    if processor.tokenizer.pad_token_id is not None:\n",
    "        inputs[\"labels\"][inputs[\"input_ids\"] == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# 4. Initialize Loader\n",
    "jsonl_file = os.path.join(DATASET_DIR, 'dataset.jsonl')\n",
    "dataset = ShaderDataset(jsonl_file)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,       # T4 Limit\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,      # Pre-load images in background\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"DataLoader ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GyuP1QwU2e8E"
   },
   "outputs": [],
   "source": [
    "# @title 5. Full Training Run (Stabilized)\n",
    "from torch.optim import AdamW\n",
    "import bitsandbytes as bnb\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Configuration\n",
    "EPOCHS = 1\n",
    "GRAD_ACCUMULATION = 4\n",
    "LEARNING_RATE = 1e-4  # LOWERED for stability (was 2e-4)\n",
    "MAX_GRAD_NORM = 1.0   # ADDED to prevent explosion\n",
    "SAVE_STEPS = 50\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# 2. Define Collator (Same as before)\n",
    "def smart_collate_fn(batch):\n",
    "    images = []\n",
    "    full_texts = []\n",
    "    prompt_only_texts = []\n",
    "\n",
    "    for item in batch:\n",
    "        try:\n",
    "            image = Image.open(item['image_path']).convert(\"RGB\")\n",
    "        except:\n",
    "            image = Image.new(\"RGB\", (256, 256), (0, 0, 0))\n",
    "        images.append(image)\n",
    "\n",
    "        full_response = item['code']\n",
    "\n",
    "        conversation_prompt = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        prompt_str = processor.apply_chat_template(conversation_prompt, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        conversation_full = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Reverse engineer the GLSL shader code for this texture. Include analysis.\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": full_response}]\n",
    "            }\n",
    "        ]\n",
    "        full_str = processor.apply_chat_template(conversation_full, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "        prompt_only_texts.append(prompt_str)\n",
    "        full_texts.append(full_str)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=full_texts,\n",
    "        images=images,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    inputs_prompts = processor(\n",
    "        text=prompt_only_texts,\n",
    "        images=images,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    for i in range(len(batch)):\n",
    "        prompt_len = inputs_prompts[\"attention_mask\"][i].sum().item()\n",
    "        prompt_len = min(prompt_len, MAX_LENGTH)\n",
    "        labels[i, :prompt_len] = -100\n",
    "        if processor.tokenizer.pad_token_id is not None:\n",
    "            labels[i][inputs[\"input_ids\"][i] == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "# 3. Create Loader\n",
    "full_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=smart_collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# 4. Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = bnb.optim.PagedAdamW8bit(params, lr=LEARNING_RATE)\n",
    "\n",
    "# 5. Training Loop\n",
    "model.train()\n",
    "# Ensure vision tower gets gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if 'visual' in name and param.requires_grad:\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(f\"  Starting STABILIZED TRAINING (LR={LEARNING_RATE}, Clip={MAX_GRAD_NORM})\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "current_loss = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(full_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # Move to GPU\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(**batch, use_cache=False)\n",
    "            loss = outputs.loss / GRAD_ACCUMULATION\n",
    "            loss.backward()\n",
    "\n",
    "            # Tracking\n",
    "            current_loss += outputs.loss.item() / GRAD_ACCUMULATION\n",
    "\n",
    "            # Update Step\n",
    "            if (step + 1) % GRAD_ACCUMULATION == 0:\n",
    "                # SAFETY: Clip Gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Logging\n",
    "                if not math.isnan(current_loss):\n",
    "                    progress_bar.set_postfix({'loss': f'{current_loss:.4f}'})\n",
    "                else:\n",
    "                    print(\"WARNING: NaN loss detected, skipping step update\")\n",
    "\n",
    "                current_loss = 0\n",
    "\n",
    "                # Saving\n",
    "                if global_step % SAVE_STEPS == 0:\n",
    "                    save_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint-{global_step}\")\n",
    "                    model.save_pretrained(save_path)\n",
    "                    processor.save_pretrained(save_path)\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"OOM at step {step}. Clearing cache...\")\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# 7. Final Save\n",
    "final_path = os.path.join(PROJECT_ROOT, \"checkpoints/stage1_final\")\n",
    "print(f\"\\nSaving FINAL STABLE model to {final_path}\")\n",
    "model.save_pretrained(final_path)\n",
    "processor.save_pretrained(final_path)\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Lz8w2HuLsK1n"
   },
   "outputs": [],
   "source": [
    "# @title 6. Auto-Shutdown\n",
    "# This cell will only run after the training cell finishes.\n",
    "import time\n",
    "from google.colab import runtime\n",
    "\n",
    "print(\"Training finished. Saving is complete.\")\n",
    "print(\"Shutting down runtime to save Compute Units in 60 seconds...\")\n",
    "\n",
    "# Give time for the final logs to sync to Drive\n",
    "time.sleep(60)\n",
    "\n",
    "print(\"Goodnight.\")\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPqY/YB2DQT76Qa3+w8xOnK",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
